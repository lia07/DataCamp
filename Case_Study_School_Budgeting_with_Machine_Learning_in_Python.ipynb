{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case Study: School Budgeting with Machine Learning in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBsSeXhAdyjql0BhnleJn/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Case_Study_School_Budgeting_with_Machine_Learning_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1idSrKDenTB"
      },
      "source": [
        "Case Study: School Budgeting with Machine Learning in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t53iRzEger4T"
      },
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5bELjDIeohf"
      },
      "source": [
        "\n",
        "1560 rows, 25 columns, 1131 non-null entries in Job_Title_Description."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED5yfhoUKlse"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Print summary statistics of the numeric columns in the DataFrame df using the .describe() method.\n",
        "Import matplotlib.pyplot as plt.\n",
        "Create a histogram of the non-null 'FTE' column. You can do this by passing df['FTE'].dropna() to plt.hist().\n",
        "The title has been specified and axes have been labeled, so hit 'Submit Answer' to see how often school employees work full-time!\n",
        "\n",
        "\n",
        " Print the summary statistics\n",
        "print(df.describe())\n",
        "\n",
        "# Import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the histogram\n",
        "plt.hist(df['FTE'].dropna())\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of %full-time \\n employee works')\n",
        "plt.xlabel('% of full-time')\n",
        "plt.ylabel('num employees')\n",
        "\n",
        "# Display the histogram\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfs0BsVOKlpy"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Define the lambda function categorize_label to convert column x into x.astype('category').\n",
        "Use the LABELS list provided to convert the subset of data df[LABELS] to categorical types using the .apply() method and categorize_label. Don't forget axis=0.\n",
        "Print the converted .dtypes attribute of df[LABELS].\n",
        "\n",
        "# Define the lambda function: categorize_label\n",
        "categorize_label = lambda x: x.astype('category')\n",
        "\n",
        "# Convert df[LABELS] to a categorical type\n",
        "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
        "\n",
        "# Print the converted dtypes\n",
        "print(df[LABELS].dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK0TixdKKlK3"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create the DataFrame num_unique_labels by using the .apply() method on df[LABELS] with pd.Series.nunique as the argument.\n",
        "Create a bar plot of num_unique_labels using pandas' .plot(kind='bar') method.\n",
        "The axes have been labeled for you, so hit 'Submit Answer' to see the number of unique values for each label.\n",
        "\n",
        "\n",
        "# Import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate number of unique values for each label: num_unique_labels\n",
        "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
        "\n",
        "# Plot number of unique values for each label\n",
        "num_unique_labels.plot(kind='bar')\n",
        "\n",
        "# Label the axes\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of unique values')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOcvJbtSegAc"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Using the compute_log_loss() function, compute the log loss for the following predicted values (in each case, the actual values are contained in actual_labels):\n",
        "correct_confident.\n",
        "correct_not_confident.\n",
        "wrong_not_confident.\n",
        "wrong_confident.\n",
        "actual_labels.\n",
        "\n",
        "\n",
        "# Compute and print log loss for 1st case\n",
        "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
        "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
        "\n",
        "# Compute log loss for 2nd case\n",
        "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
        "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for 3rd case\n",
        "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
        "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for 4th case\n",
        "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
        "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for actual labels\n",
        "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
        "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9KsL6BvgpQW"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWVil4v0ef8q"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a new DataFrame named numeric_data_only by applying the .fillna(-1000) method to the numeric columns (available in the list NUMERIC_COLUMNS) of df.\n",
        "Convert the labels (available in the list LABELS) to dummy variables. Save the result as label_dummies.\n",
        "In the call to multilabel_train_test_split(), set the size of your test set to be 0.2. Use a seed of 123.\n",
        "Fill in the .info() method calls for X_train, X_test, y_train, and y_test.\n",
        "\n",
        "\n",
        "# Create the new DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
        "                                                               label_dummies,\n",
        "                                                               size=0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Print the info\n",
        "print(\"X_train info:\")\n",
        "print(X_train.info())\n",
        "print(\"\\nX_test info:\")  \n",
        "print(X_test.info())\n",
        "print(\"\\ny_train info:\")  \n",
        "print(y_train.info())\n",
        "print(\"\\ny_test info:\")  \n",
        "print(y_test.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT9Z_5Tuefp6"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import LogisticRegression from sklearn.linear_model and OneVsRestClassifier from sklearn.multiclass.\n",
        "Instantiate the classifier clf by placing LogisticRegression() inside OneVsRestClassifier().\n",
        "Fit the classifier to the training data X_train and y_train.\n",
        "Compute and print the accuracy of the classifier using its .score() method, which accepts two arguments: X_test and y_test.\n",
        "\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Create the DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
        "                                                               label_dummies,\n",
        "                                                               size=0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-DswdJmerT_"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Read HoldoutData.csv into a DataFrame called holdout. Specify the keyword argument index_col=0 in your call to read_csv().\n",
        "Generate predictions using .predict_proba() on the numeric columns (available in the NUMERIC_COLUMNS list) of holdout. Make sure to fill in missing values with -1000!\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit it to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Load the holdout data: holdout\n",
        "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
        "\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwK_JQVoerR8"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Read HoldoutData.csv into a DataFrame called holdout. Specify the keyword argument index_col=0 in your call to read_csv().\n",
        "Generate predictions using .predict_proba() on the numeric columns (available in the NUMERIC_COLUMNS list) of holdout. Make sure to fill in missing values with -1000!\n",
        "\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit it to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Load the holdout data: holdout\n",
        "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
        "\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQDs_k4Jeq88"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create the prediction_df DataFrame by specifying the following arguments to the provided parameters pd.DataFrame():\n",
        "pd.get_dummies(df[LABELS]).columns.\n",
        "holdout.index.\n",
        "predictions.\n",
        "Save prediction_df to a csv file called 'predictions.csv' using the .to_csv() method.\n",
        "Submit the predictions for scoring by using the score_submission() function with pred_path set to 'predictions.csv'.\n",
        "\n",
        "\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
        "\n",
        "# Format predictions in DataFrame: prediction_df\n",
        "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
        "                             index=holdout.index,\n",
        "                             data=predictions)\n",
        "\n",
        "\n",
        "# Save prediction_df to csv\n",
        "prediction_df.to_csv('predictions.csv')\n",
        "\n",
        "# Submit the predictions for scoring: score\n",
        "score = score_submission(pred_path='predictions.csv')\n",
        "\n",
        "# Print score\n",
        "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8PfU0JU4tEm"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import CountVectorizer from sklearn.feature_extraction.text.\n",
        "Fill missing values in df.Position_Extra using .fillna('') to replace NaNs with empty strings. Specify the additional keyword argument inplace=True so that you don't have to assign the result back to df.\n",
        "Instantiate the CountVectorizer as vec_alphanumeric by specifying the token_pattern to be TOKENS_ALPHANUMERIC.\n",
        "Fit vec_alphanumeric to df.Position_Extra.\n",
        "Hit 'Submit Answer' to see the len of the fitted representation as well as the first 15 elements, and compare to vec_basic.\n",
        "\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Fill missing values in df.Position_Extra\n",
        "df.Position_Extra.fillna('', inplace=True)\n",
        "\n",
        "# Instantiate the CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit to the data\n",
        "vec_alphanumeric.fit(df.Position_Extra)\n",
        "\n",
        "# Print the number of tokens and first 15 tokens\n",
        "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
        "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
        "print(vec_alphanumeric.get_feature_names()[:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q28q-eTC47o7"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Use the .drop() method on data_frame with to_drop and axis= as arguments to drop the non-text data. Save the result as text_data.\n",
        "Fill in missing values (inplace) in text_data with blanks (\"\"), using the .fillna() method.\n",
        "Complete the .apply() method by writing a lambda function that uses the .join() method to join all the items in a row with a space in between.\n",
        "\n",
        "# Define combine_text_columns()\n",
        "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
        "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
        "    \n",
        "    # Drop non-text columns that are in the df\n",
        "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
        "    text_data = data_frame.drop(to_drop, axis=1)\n",
        "    \n",
        "    # Replace nans with blanks\n",
        "    text_data.fillna(\"\", inplace=True)\n",
        "    \n",
        "    # Join all text items in a row that have a space in between\n",
        "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UajAnkjw47Yc"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import CountVectorizer from sklearn.feature_extraction.text.\n",
        "Instantiate vec_basic and vec_alphanumeric using, respectively, the TOKENS_BASIC and TOKENS_ALPHANUMERIC patterns.\n",
        "Create the text vector by using the combine_text_columns() function on df.\n",
        "Using the .fit_transform() method with text_vector, fit and transform first vec_basic and then vec_alphanumeric. Print the number of tokens they contain.\n",
        "\n",
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the basic token pattern\n",
        "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
        "\n",
        "# Create the alphanumeric token pattern\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate basic CountVectorizer: vec_basic\n",
        "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
        "\n",
        "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(df)\n",
        "\n",
        "# Fit and transform vec_basic\n",
        "vec_basic.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_basic\n",
        "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
        "\n",
        "# Fit and transform vec_alphanumeric\n",
        "vec_alphanumeric.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_alphanumeric\n",
        "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgTfO4lT5dJR"
      },
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USjWsHf85T0L"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import Pipeline from sklearn.pipeline.\n",
        "Create training and test sets using the numeric data only. Do this by specifying sample_df[['numeric']] in train_test_split().\n",
        "Instantiate a pipeline as pl by adding the classifier step. Use a name of 'clf' and the same classifier from Chapter 2: OneVsRestClassifier(LogisticRegression()).\n",
        "Fit your pipeline to the training data and compute its accuracy to see it in action! Since this is toy data, you'll use the default scoring method for now. In the next chapter, you'll return to log loss scoring.\n",
        "\n",
        "\n",
        "# Import Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import other necessary modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Split and select numeric data only, no nans \n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=22)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVhmvSBX47Sl"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import Imputer from sklearn.preprocessing.\n",
        "Create training and test sets by selecting the correct subset of sample_df: 'numeric' and 'with_missing'.\n",
        "Add the tuple ('imp', Imputer()) to the correct position in the pipeline. Pipeline processes steps sequentially, so the imputation step should come before the classifier step.\n",
        "Complete the .fit() and .score() methods to fit the pipeline to the data and compute the accuracy.\n",
        "\n",
        "# Import the Imputer object\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "# Create training and test sets using only numeric data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Insantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('imp', Imputer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckkqkvvcr7Kf"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import CountVectorizer from sklearn.feature_extraction.text.\n",
        "Create training and test sets by selecting the correct subset of sample_df: 'text'.\n",
        "Add the CountVectorizer step (with the name 'vec') to the correct position in the pipeline.\n",
        "Fit the pipeline to the training data and compute its accuracy.\n",
        "\n",
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Split out only the text data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('vec', CountVectorizer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF19n_mzzWFx"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Compute the selector get_text_data by using a lambda function and FunctionTransformer() to obtain all 'text' columns.\n",
        "Compute the selector get_numeric_data by using a lambda function and FunctionTransformer() to obtain all the numeric columns (including missing data). These are 'numeric' and 'with_missing'.\n",
        "Fit and transform get_text_data using the .fit_transform() method with sample_df as the argument.\n",
        "Fit and transform get_numeric_data using the same approach as above.\n",
        "\n",
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Obtain the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
        "\n",
        "# Obtain the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
        "\n",
        "# Fit and transform the text data: just_text_data\n",
        "just_text_data = get_text_data.fit_transform(sample_df)\n",
        "\n",
        "# Fit and transform the numeric data: just_numeric_data\n",
        "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
        "\n",
        "# Print head to check results\n",
        "print('Text Data')\n",
        "print(just_text_data.head())\n",
        "print('\\nNumeric Data')\n",
        "print(just_numeric_data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWnf3o8dr6tY"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "In the process_and_join_features:\n",
        "Add the steps ('selector', get_numeric_data) and ('imputer', Imputer()) to the 'numeric_features' preprocessing step.\n",
        "Add the equivalent steps for the text_features preprocessing step. That is, use get_text_data and a CountVectorizer step with the name 'vectorizer'.\n",
        "Add the transform step process_and_join_features to 'union' in the main pipeline, pl.\n",
        "Hit 'Submit Answer' to see the pipeline in action!\n",
        "\n",
        "# Import FeatureUnion\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Split using ALL data in sample_df\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=22)\n",
        "\n",
        "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
        "process_and_join_features = FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )\n",
        "\n",
        "# Instantiate nested pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', process_and_join_features),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "\n",
        "# Fit pl to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IEmqkkyr6dR"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Complete the call to multilabel_train_test_split() by selecting df[NON_LABELS].\n",
        "Compute get_text_data by using FunctionTransformer() and passing in combine_text_columns. Be sure to also specify validate=False.\n",
        "Use FunctionTransformer() to compute get_numeric_data. In the lambda function, select out the NUMERIC_COLUMNS of x. Like you did when computing get_text_data, also specify validate=False.\n",
        "\n",
        "\n",
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Get the dummy encoding of the labels\n",
        "dummy_labels = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Get the columns that are features in the original df\n",
        "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
        "                                                               dummy_labels,\n",
        "                                                               0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Preprocess the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "\n",
        "# Preprocess the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwRn9Fkh4fdS"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Complete the 'numeric_features' transform with the following steps:\n",
        "get_numeric_data, with the name 'selector'.\n",
        "Imputer(), with the name 'imputer'.\n",
        "Complete the 'text_features' transform with the following steps:\n",
        "get_text_data, with the name 'selector'.\n",
        "CountVectorizer(), with the name 'vectorizer'.\n",
        "Fit the pipeline to the training data.\n",
        "Hit 'Submit Answer' to compute the accuracy!\n",
        "\n",
        "# Complete the pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXyaYnRU4fIN"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import the RandomForestClassifier from sklearn.ensemble.\n",
        "Add a RandomForestClassifier() step named 'clf' to the pipeline.\n",
        "Hit 'Submit Answer' to fit the pipeline to the training data and compute its accuracy.\n",
        "\n",
        "# Import random forest classifer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Edit model step in pipeline\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRQs8LEm7RsL"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import the RandomForestClassifier from sklearn.ensemble.\n",
        "Add a RandomForestClassifier() step with n_estimators=15 to the pipeline with a name of 'clf'.\n",
        "Hit 'Submit Answer' to fit the pipeline to the training data and compute its accuracy.\n",
        "\n",
        "\n",
        "\n",
        "# Import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Add model step to pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', RandomForestClassifier(n_estimators=15))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku1Jo3MJ8F1Y"
      },
      "source": [
        "4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dH2OJNx8Hir"
      },
      "source": [
        "4, because , and & are not tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Spl_s_-7RoS"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create text_vector by preprocessing X_train using combine_text_columns. This is important, or else you won't get any tokens!\n",
        "Instantiate CountVectorizer as text_features. Specify the keyword argument token_pattern=TOKENS_ALPHANUMERIC.\n",
        "Fit text_features to the text_vector.\n",
        "Hit 'Submit Answer' to print the first 10 tokens.\n",
        "\n",
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate the CountVectorizer: text_features\n",
        "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit text_features to the text vector\n",
        "text_features.fit(text_vector)\n",
        "\n",
        "# Print the first 10 tokens\n",
        "print(text_features.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxmMRX-n7RUA"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import CountVectorizer from sklearn.feature_extraction.text.\n",
        "Add a CountVectorizer step to the pipeline with the name 'vectorizer'.\n",
        "Set the token pattern to be TOKENS_ALPHANUMERIC.\n",
        "Set the ngram_range to be (1, 2).\n",
        "\n",
        "\n",
        "# Import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import other preprocessing modules\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "\n",
        "# Select 300 best features\n",
        "chi_k = 300\n",
        "\n",
        "# Import functional utilities\n",
        "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Perform preprocessing\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXk6_BsN8hbn"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Add the interaction terms step using SparseInteractions() with degree=2. Give it a name of 'int', and make sure it is after the preprocessing step but before scaling.\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, \n",
        "                                                   ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),  \n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWImFS2I7RPW"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import HashingVectorizer from sklearn.feature_extraction.text.\n",
        "Instantiate the HashingVectorizer as hashing_vec using the TOKENS_ALPHANUMERIC pattern.\n",
        "Fit and transform hashing_vec using text_data. Save the result as hashed_text.\n",
        "Hit 'Submit Answer' to see some of the resulting hash values.\n",
        "\n",
        " Import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Get text data: text_data\n",
        "text_data = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
        "\n",
        "# Instantiate the HashingVectorizer: hashing_vec\n",
        "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit and transform the Hashing Vectorizer\n",
        "hashed_text = hashing_vec.fit_transform(text_data)\n",
        "\n",
        "# Create DataFrame and print the head\n",
        "hashed_df = pd.DataFrame(hashed_text.data)\n",
        "print(hashed_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMXBSX4U4ezr"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import HashingVectorizer from sklearn.feature_extraction.text.\n",
        "Add a HashingVectorizer step to the pipeline.\n",
        "Name the step 'vectorizer'.\n",
        "Use the TOKENS_ALPHANUMERIC token pattern.\n",
        "Specify the ngram_range to be (1, 2)\n",
        "\n",
        "# Import the hashing vectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Instantiate the winning model pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                     non_negative=True, norm=None, binary=False,\n",
        "                                                     ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c2Nklc9-cZz"
      },
      "source": [
        "What tactics got the winner the best score?\n",
        "Now you've implemented the winning model from start to finish. If you want to use this model locally, this Jupyter notebook contains all the code you've worked so hard on. You can now take that code and build on it!\n",
        "\n",
        "Let's take a moment to reflect on why this model did so well. What tactics got the winner the best score?\n",
        "\n",
        "\n",
        "\n",
        "The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1XydsWV-cSd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysxa1xDN-cNU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqe2N2Bf-cIa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g2-HMgO-cBR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bby_5eW47Ca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}