{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Data_in_Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrLMFFik78iQp76BLBIcYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Cleaning_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhY6RjrbuU8R"
      },
      "source": [
        "Cleaning Data in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee57CeSuLz3C"
      },
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf-Z4rrDua-h"
      },
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Print the information of ride_sharing.\n",
        "Use .describe() to print the summary statistics of the user_type column from ride_sharing.\n",
        "\n",
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Question\n",
        "By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?\n",
        "\n",
        "The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category.\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Convert user_type into categorical by assigning it the 'category' data type and store it in the user_type_cat column.\n",
        "Make sure you converted user_type_cat correctly by using an assert statement.\n",
        "\n",
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "# Convert user_type from integer to category\n",
        "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
        "\n",
        "# Write an assert statement confirming the change\n",
        "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
        "\n",
        "# Print new summary statistics \n",
        "print(ride_sharing['user_type_cat'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nkxu_9NyTAU"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Use the .strip() method to strip duration of \"minutes\" and store it in the duration_trim column.\n",
        "Convert duration_trim to int and store it in the duration_time column.\n",
        "Write an assert statement that checks if duration_time's data type is now an int.\n",
        "Print the average ride duration.\n",
        "\n",
        "# Strip duration of minutes\n",
        "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes') \n",
        "\n",
        "# Convert duration to integer\n",
        "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
        "\n",
        "# Write an assert statement making sure of conversion\n",
        "assert ride_sharing['duration_time'].dtype == 'int'\n",
        "\n",
        "# Print formed columns and calculate average ride duration \n",
        "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
        "print(ride_sharing['duration_time'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsXs-36Mwju7"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Convert the tire_sizes column from category to 'int'.\n",
        "Use .loc[] to set all values of tire_sizes above 27 to 27.\n",
        "Reconvert back tire_sizes to 'category' from int.\n",
        "Print the description of the tire_sizes.\n",
        "\n",
        "# Convert tire_sizes to integer\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
        "\n",
        "# Set all values above 27 to 27\n",
        "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
        "\n",
        "# Reconvert tire_sizes back to categorical\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
        "\n",
        "# Print tire size description\n",
        "print(ride_sharing['tire_sizes'].describe())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2q9XDQ_tuj"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Convert ride_date to a datetime object and store it in ride_dt column using to_datetime().\n",
        "Create the variable today, which stores today's date by using the dt.date.today() function.\n",
        "For all instances of ride_dt in the future, set them to today's date.\n",
        "Print the maximum date in the ride_dt column.\n",
        "\n",
        "# Convert ride_date to datetime\n",
        "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
        "\n",
        "# Save today's date\n",
        "today = dt.date.today()\n",
        "\n",
        "# Set all in the future to today's date\n",
        "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
        "\n",
        "# Print maximum of ride_dt column\n",
        "print(ride_sharing['ride_dt'].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN92GaEu_7Gm"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.\n",
        "Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.\n",
        "Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order.\n",
        "\n",
        "# Find duplicates\n",
        "duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
        "\n",
        "# Sort your duplicated rides\n",
        "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
        "\n",
        "# Print relevant columns\n",
        "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7bBXMlOI0kC"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Drop complete duplicates in ride_sharing and store the results in ride_dup.\n",
        "Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.\n",
        "Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.\n",
        "Find duplicates again and run the assert statement to verify de-duplication.\n",
        "\n",
        "# Drop complete duplicates from ride_sharing\n",
        "ride_dup = ride_sharing.drop_duplicates()\n",
        "\n",
        "# Create statistics dictionary for aggregation function\n",
        "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
        "\n",
        "# Group by ride_id and compute new statistics\n",
        "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
        "\n",
        "# Find duplicated values again\n",
        "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
        "duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "# Assert duplicates are processed\n",
        "assert duplicated_rides.shape[0] == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2bHGyYpLwrD"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY_2wt6lLp-P"
      },
      "source": [
        "Instructions 1/4\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.\n",
        "Print the unique values of the survey columns in airlines using the .unique() method.\n",
        "\n",
        "# Print categories DataFrame\n",
        "print(categories)\n",
        "\n",
        "# Print unique values of survey columns in airlines\n",
        "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', airlines['satisfaction'].unique(),\"\\n\")\n",
        "\n",
        "Instructions 2/4\n",
        "15 XP\n",
        "3\n",
        "4\n",
        "Question\n",
        "Take a look at the output. Out of the cleanliness, safety and satisfaction columns, which one has an inconsistent category and what is it?\n",
        "\n",
        "Possible Answers\n",
        "\n",
        "cleanliness because it has an Unacceptable category.\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
        "Find rows of airlines with a cleanliness value not in categories and print the output.\n",
        "\n",
        "# Find the cleanliness category in airlines not in categories\n",
        "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
        "\n",
        "# Find rows with that category\n",
        "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Print rows with inconsistent category\n",
        "print(airlines[cat_clean_rows])\n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Print the rows with the consistent categories of cleanliness only.\n",
        "\n",
        "# Find the cleanliness category in airlines not in categories\n",
        "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
        "\n",
        "# Find rows with that category\n",
        "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Print rows with inconsistent category\n",
        "print(airlines[cat_clean_rows])\n",
        "\n",
        "# Print rows with consistent categories only\n",
        "print(airlines[~cat_clean_rows])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY4QUsRFulgR"
      },
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Print the unique values in dest_region and dest_size respectively.\n",
        "\n",
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].value_counts())\n",
        "print(airlines['dest_region'].unique())\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Question\n",
        "From looking at the output, what do you think is the problem with these columns?\n",
        "\n",
        "Both 2 and 3 are correct.\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Change the capitalization of all values of dest_region to lowercase.\n",
        "Replace the 'eur' with 'europe' in dest_region using the .replace() method.\n",
        "\n",
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'}\n",
        "                                                          \n",
        "                                                          Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Strip white spaces from the dest_size column using the .strip() method.\n",
        "Verify that the changes have been into effect by printing the unique values of the columns using .unique() .\n",
        "\n",
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
        "\n",
        "# Remove white spaces from `dest_size`\n",
        "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
        "\n",
        "# Verify changes have been effected\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "                                                          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTSZly2wy26y"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create the ranges and labels for the wait_type column mentioned in the description above.\n",
        "Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.\n",
        "Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.\n",
        "Create the day_week column by using .replace().\n",
        "\n",
        "# Create ranges for categories\n",
        "label_ranges = [0, 60, 180, np.inf]\n",
        "label_names = ['short', 'medium', 'long']\n",
        "\n",
        "# Create wait_type column\n",
        "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
        "                               labels = label_names)\n",
        "\n",
        "# Create mappings and replace\n",
        "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
        "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
        "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
        "\n",
        "airlines['day_week'] = airlines['day'].replace(mappings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34uoVdNr1cBe"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Remove \"Dr.\", \"Mr.\", \"Miss\" and \"Ms.\" from full_name by replacing them with an empty string \"\" in that order.\n",
        "Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics.\n",
        "\n",
        "# Replace \"Dr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\", \"\")\n",
        "\n",
        "# Replace \"Mr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\", \"\")\n",
        "\n",
        "# Replace \"Miss\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\", \"\")\n",
        "\n",
        "# Replace \"Ms.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\", \"\")\n",
        "\n",
        "# Assert that full_name has no honorifics\n",
        "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5q4FwMa6on1"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().\n",
        "Isolate the rows of airlines with resp_length higher than 40.\n",
        "Assert that the smallest survey response length in airlines_survey is now bigger than 40.\n",
        "\n",
        "# Store length of each row in survey_response column\n",
        "resp_length = airlines['survey_response'].str.len()\n",
        "\n",
        "# Find rows in airlines where resp_length > 40\n",
        "airlines_survey = airlines[resp_length > 40]\n",
        "\n",
        "# Assert minimum survey_response length is > 40\n",
        "assert airlines_survey['survey_response'].str.len().min() > 40\n",
        "\n",
        "# Print new survey_response column\n",
        "print(airlines_survey['survey_response'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7UUwHo091r8"
      },
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUuOMA4Q8oEM"
      },
      "source": [
        "Instrucciones\n",
        "100 XP\n",
        "Encuentra las filas de acct_curin bankingque son iguales a 'euro'y guárdalas acct_eu.\n",
        "Encuentre todas las filas de acct_amountin bankingque se ajusten a la acct_eucondición y conviértalas a USD multiplicándolas por 1.1.\n",
        "Busque todas las filas de acct_curin bankingque se ajusten a la acct_eucondición, configúrelas en 'dollar'.\n",
        "\n",
        "# Find values of acct_cur that are equal to 'euro'\n",
        "acct_eu = banking['acct_cur'] == 'euro'\n",
        "\n",
        "# Convert acct_amount where it is in euro to dollars\n",
        "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
        "\n",
        "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
        "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
        "\n",
        "# Assert that only dollar currency remains\n",
        "assert banking['acct_cur'].unique() == 'dollar'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kMkUqX09sjU"
      },
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Print the header of account_opened from the banking DataFrame and take a look at the different results.\n",
        "\n",
        "# Print the header of account_opened\n",
        "print(banking['account_opened'].head())\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.\n",
        "\n",
        "# Print the header of account_opened\n",
        "print(banking['account_opened'].head())\n",
        "\n",
        "# Convert account_opened to datetime\n",
        "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
        "                                           # Infer datetime format\n",
        "                                           infer_datetime_format = True,\n",
        "                                           # Return missing value for error\n",
        "                                           errors = 'coerce') \n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Extract the year from the amended account_opened column and assign it to the acct_year column.\n",
        "Print the newly created acct_year column.\n",
        "\n",
        "# Print the header of account_opend\n",
        "print(banking['account_opened'].head())\n",
        "\n",
        "# Convert account_opened to datetime\n",
        "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
        "                                           # Infer datetime format\n",
        "                                           infer_datetime_format = True,\n",
        "                                           # Return missing value for error\n",
        "                                           errors = 'coerce')  \n",
        "\n",
        "# Get year of account opened\n",
        "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
        "\n",
        "# Print acct_year\n",
        "print(banking['acct_year'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCOb79yxBok2"
      },
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
        "Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.\n",
        "\n",
        "# Store fund columns to sum against\n",
        "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
        "\n",
        "# Find rows where fund_columns row sum == inv_amount\n",
        "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "consistent_inv = banking[inv_equ]\n",
        "inconsistent_inv = banking[~inv_equ]\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
        "Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.\n",
        "2\n",
        "Store today's date into today, and manually calculate customers' ages and store them in ages_manual.\n",
        "Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages.\n",
        "\n",
        "# Store today's date and find ages\n",
        "today = dt.date.today()\n",
        "ages_manual = today.year - banking['birth_date'].dt.year\n",
        "\n",
        "# Find rows where age column == ages_manual\n",
        "age_equ = banking['age'] == ages_manual\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "consistent_ages = banking[age_equ]\n",
        "inconsistent_ages = banking[~age_equ]\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFHXPylCCNW3"
      },
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Print the number of missing values by column in the banking DataFrame.\n",
        "Plot and show the missingness matrix of banking with the msno.matrix() function.\n",
        "\n",
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors.\n",
        "\n",
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()\n",
        "\n",
        "# Isolate missing and non missing values of inv_amount\n",
        "missing_investors = banking[banking['inv_amount'].isna()]\n",
        "investors = banking[~banking['inv_amount'].isna()]\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Question\n",
        "Now that you've isolated banking into investors and missing_investors, use the .describe() method on both of these DataFrames in the console to understand whether there are structural differences between them. What do you think is going on?\n",
        "\n",
        "The inv_amount is missing only for young customers, since the average age in missing_investors is 22 and the maximum age is 25.\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted.\n",
        "\n",
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()\n",
        "\n",
        "# Isolate missing and non missing values of inv_amount\n",
        "missing_investors = banking[banking['inv_amount'].isna()]\n",
        "investors = banking[~banking['inv_amount'].isna()]\n",
        "\n",
        "# Sort banking by age and visualize\n",
        "banking_sorted = banking.sort_values(by = 'age')\n",
        "msno.matrix(banking_sorted)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFeJ5JXrZ5o4"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid.\n",
        "Compute the estimated acct_amount of banking_fullid knowing that acct_amount is usually inv_amount * 5 and assign the results to acct_imp.\n",
        "Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna().\n",
        "\n",
        "# Drop missing values of cust_id\n",
        "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
        "\n",
        "# Compute estimated acct_amount\n",
        "acct_imp = banking_fullid['inv_amount'] * 5\n",
        "\n",
        "# Impute missing acct_amount with corresponding acct_imp\n",
        "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
        "\n",
        "# Print number of missing values\n",
        "print(banking_imputed.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddigWpQvyS5i"
      },
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Import process from fuzzywuzzy.\n",
        "Store the unique cuisine_types into unique_types.\n",
        "Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all possible matches.\n",
        "\n",
        "# Import process from fuzzywuzzy\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Store the unique values of cuisine_type in unique_types\n",
        "unique_types = restaurants['cuisine_type'].unique()\n",
        "\n",
        "# Calculate similarity of 'asian' to all values of unique_types\n",
        "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# Calculate similarity of 'american' to all values of unique_types\n",
        "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# Calculate similarity of 'italian' to all values of unique_types\n",
        "print(process.extract('italian', unique_types, limit = len(unique_types)))\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Question\n",
        "Take a look at the output, what do you think should be the similarity cutoff point when remapping categories?\n",
        "\n",
        "Possible Answers\n",
        "\n",
        "80\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIR_w-RB76k6"
      },
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Return all of the unique values in the cuisine_type column of restaurants.\n",
        "\n",
        "# Inspect the unique values of the cuisine_type column\n",
        "print(restaurants['cuisine_type'].unique())\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Okay! Looks like you will need to use some string matching to correct these misspellings!\n",
        "\n",
        "As a first step, create a list of matches, comparing 'italian' with the restaurant types listed in the cuisine_type column.\n",
        "\n",
        "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
        "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "# Inspect the first 5 matches\n",
        "print(matches[0:5])\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Now you're getting somewhere! Now you can iterate through matches to reassign similar entries.\n",
        "\n",
        "Within the for loop, use an if statement to check whether the similarity score in each match is greater than or equal to 80.\n",
        "If it is, use .loc to select rows where cuisine_type in restaurants is equal to the current match (which is the first element of match), and reassign them to be 'italian'.\n",
        "\n",
        "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
        "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "# Iterate through the list of matches to italian\n",
        "for match in matches:\n",
        "  # Check whether the similarity score is greater than or equal to 80\n",
        "  if match[1] >= 80:\n",
        "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'\n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Finally, you'll adapt your code to work with every restaurant type in categories.\n",
        "\n",
        "Using the variable cuisine to iterate through categories, embed your code from the previous step in an outer for loop.\n",
        "Inspect the final result. This has been done for you.\n",
        "\n",
        "# Iterate through categories\n",
        "for cuisine in categories:  \n",
        "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
        "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "  # Iterate through the list of matches\n",
        "  for match in matches:\n",
        "     # Check whether the similarity score is greater than or equal to 80\n",
        "    if match[1] >= 80:\n",
        "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
        "      \n",
        "# Inspect the final result\n",
        "print(restaurants['cuisine_type'].unique())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNzjd06dGm6q"
      },
      "source": [
        " Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Instantiate an indexing object by using the Index() function from recordlinkage.\n",
        "Block your pairing on cuisine_type by using indexer's' .block() method.\n",
        "Generate pairs by indexing restaurants and restaurants_new in that order.\n",
        "\n",
        "# Create an indexer and object and find possible pairs\n",
        "indexer = recordlinkage.Index()\n",
        "\n",
        "# Block pairing on cuisine_type\n",
        "indexer.block('cuisine_type')\n",
        "\n",
        "# Generate pairs\n",
        "pairs = indexer.index(restaurants, restaurants_new)\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Question\n",
        "Now that you've generated your pairs, you've achieved the first step of record linkage. What are the steps remaining to link both restaurants DataFrames, and in what order?\n",
        "\n",
        "\n",
        "Compare between columns, score the comparison, then link the DataFrames.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Instantiate a comparison object using the recordlinkage.Compare() function.\n",
        "# Create a comparison object\n",
        "comp_cl = recordlinkage.Compare()\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames.\n",
        "Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.\n",
        "\n",
        "# Create a comparison object\n",
        "comp_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches on city, cuisine_types \n",
        "comp_cl.exact('city', 'city', label='city')\n",
        "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
        "\n",
        "# Find similar matches of rest_name\n",
        "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Compute the comparison of the pairs by using the .compute() method of comp_cl.\n",
        "\n",
        "# Create a comparison object\n",
        "comp_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches on city, cuisine_types - \n",
        "comp_cl.exact('city', 'city', label='city')\n",
        "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
        "\n",
        "# Find similar matches of rest_name\n",
        "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
        "\n",
        "# Get potential matches and print\n",
        "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
        "print(potential_matches)\n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Question\n",
        "Print out potential_matches, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with\n",
        "\n",
        "potential_matches[potential_matches.sum(axis = 1) >= n]\n",
        "Where n is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of n be?\n",
        "\n",
        "Possible Answers\n",
        "\n",
        "3 because I need to have matches in all my co\n",
        "\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.\n",
        "Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.\n",
        "Subset restaurants_new for rows that are not in matching_indices.\n",
        "Append non_dup to restaurants.\n",
        "\n",
        "# Isolate potential matches with row sum >=3\n",
        "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
        "\n",
        "# Get values of second column index of matches\n",
        "matching_indices = matches.index.get_level_values(1)\n",
        "\n",
        "# Subset restaurants_new based on non-duplicate values\n",
        "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
        "\n",
        "# Append non_dup to restaurants\n",
        "full_restaurants = restaurants.append(non_dup)\n",
        "print(full_restaurants)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kIkS02BtMQG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}