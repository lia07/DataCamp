{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJUO5Tx+sg8SNz8PinwBa0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Unsupervised_Learning_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkRk-adoA8XN"
      },
      "source": [
        "Unsupervised Learning in Python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8_OQP5iA9kE"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import KMeans from sklearn.cluster.\n",
        "Using KMeans(), create a KMeans instance called model to find 3 clusters. To specify the number of clusters, use the n_clusters keyword argument.\n",
        "Use the .fit() method of model to fit the model to the array of points points.\n",
        "Use the .predict() method of model to predict the cluster labels of new_points, assigning the result to labels.\n",
        "Hit 'Submit Answer' to see the cluster labels of new_points.\n",
        "\n",
        "# Import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans instance with 3 clusters: model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(points)\n",
        "\n",
        "# Determine the cluster labels of new_points: labels\n",
        "labels = model.predict(new_points)\n",
        "\n",
        "# Print cluster labels of new_points\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIluXhG_IOrX"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import matplotlib.pyplot as plt.\n",
        "Assign column 0 of new_points to xs, and column 1 of new_points to ys.\n",
        "Make a scatter plot of xs and ys, specifying the c=labels keyword arguments to color the points by their cluster label. Also specify alpha=0.5.\n",
        "Compute the coordinates of the centroids using the .cluster_centers_ attribute of model.\n",
        "Assign column 0 of centroids to centroids_x, and column 1 of centroids to centroids_y.\n",
        "Make a scatter plot of centroids_x and centroids_y, using 'D' (a diamond) as a marker by specifying the marker parameter. Set the size of the markers to be 50 using s=50.\n",
        "\n",
        "\n",
        "# Import pyplot\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Assign the columns of new_points: xs and ys\n",
        "xs = new_points[:,0]\n",
        "ys = new_points[:,1]\n",
        "\n",
        "# Make a scatter plot of xs and ys, using labels to define the colors\n",
        "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
        "\n",
        "# Assign the cluster centers: centroids\n",
        "centroids = model.cluster_centers_\n",
        "\n",
        "# Assign the columns of centroids: centroids_x, centroids_y\n",
        "centroids_x = centroids[:,0]\n",
        "centroids_y = centroids[:,1]\n",
        "\n",
        "# Make a scatter plot of centroids_x and centroids_y\n",
        "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aC7ceAURRds"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Instructions\n",
        "100 XP\n",
        "For each of the given values of k, perform the following steps:\n",
        "Create a KMeans instance called model with k clusters.\n",
        "Fit the model to the grain data samples.\n",
        "Append the value of the inertia_ attribute of model to the list inertias.\n",
        "The code to plot ks vs inertias has been written for you, so hit 'Submit Answer' to see the plot!\n",
        "\n",
        "ks = range(1, 6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters=k)\n",
        "    \n",
        "    # Fit model to samples\n",
        "    model.fit(samples)\n",
        "    \n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVPTmYDfUfmH"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a KMeans model called model with 3 clusters.\n",
        "Use the .fit_predict() method of model to fit it to samples and derive the cluster labels. Using .fit_predict() is the same as using .fit() followed by .predict().\n",
        "Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\n",
        "Use the pd.crosstab() function on df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label. Assign the result to ct.\n",
        "Hit 'Submit Answer' to see the cross-tabulation!\n",
        "\n",
        "# Create a KMeans model with 3 clusters: model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Use fit_predict to fit model and obtain cluster labels: labels\n",
        "labels = model.fit_predict(samples)\n",
        "\n",
        "# Create a DataFrame with clusters and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0l-FNihUq7V"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "make_pipeline from sklearn.pipeline.\n",
        "StandardScaler from sklearn.preprocessing.\n",
        "KMeans from sklearn.cluster.\n",
        "Create an instance of StandardScaler called scaler.\n",
        "Create an instance of KMeans with 4 clusters called kmeans.\n",
        "Create a pipeline called pipeline that chains scaler and kmeans. To do this, you just need to pass them in as arguments to make_pipeline().\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru2iQRjIU7Vp"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Fit the pipeline to the fish measurements samples.\n",
        "Obtain the cluster labels for samples by using the .predict() method of pipeline.\n",
        "Using pd.DataFrame(), create a DataFrame df with two columns named 'labels' and 'species', using labels and species, respectively, for the column values.\n",
        "Using pd.crosstab(), create a cross-tabulation ct of df['labels'] and df['species'].\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to samples\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and species as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'species': species})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['species'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D01nlrvTVKEV"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import Normalizer from sklearn.preprocessing.\n",
        "Create an instance of Normalizer called normalizer.\n",
        "Create an instance of KMeans called kmeans with 10 clusters.\n",
        "Using make_pipeline(), create a pipeline called pipeline that chains normalizer and kmeans.\n",
        "Fit the pipeline to the movements array.\n",
        "\n",
        "\n",
        "# Import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a KMeans model with 10 clusters: kmeans\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "\n",
        "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
        "pipeline = make_pipeline(normalizer, kmeans)\n",
        "\n",
        "# Fit pipeline to the daily price movements\n",
        "pipeline.fit(movements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7C2-soYVPxD"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Use the .predict() method of the pipeline to predict the labels for movements.\n",
        "Align the cluster labels with the list of company names companies by creating a DataFrame df with labels and companies as columns. This has been done for you.\n",
        "Use the .sort_values() method of df to sort the DataFrame by the 'labels' column, and print the result.\n",
        "Hit 'Submit Answer' and take a moment to see which companies are together in each cluster!\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Predict the cluster labels: labels\n",
        "labels = pipeline.predict(movements)\n",
        "\n",
        "# Create a DataFrame aligning labels and companies: df\n",
        "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('labels'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6o0-6YDVdDV"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1udSoPauVeiD"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "linkage and dendrogram from scipy.cluster.hierarchy.\n",
        "matplotlib.pyplot as plt.\n",
        "Perform hierarchical clustering on samples using the linkage() function with the method='complete' keyword argument. Assign the result to mergings.\n",
        "Plot a dendrogram using the dendrogram() function on mergings. Specify the keyword arguments labels=varieties, leaf_rotation=90, and leaf_font_size=6.\n",
        "\n",
        "# Perform the necessary imports\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='complete')\n",
        "\n",
        "# Plot the dendrogram, using varieties as labels\n",
        "dendrogram(mergings,\n",
        "           labels=varieties,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6,\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzOd_mFpWorP"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import normalize from sklearn.preprocessing.\n",
        "Rescale the price movements for each stock by using the normalize() function on movements.\n",
        "Apply the linkage() function to normalized_movements, using 'complete' linkage, to calculate the hierarchical clustering. Assign the result to mergings.\n",
        "Plot a dendrogram of the hierarchical clustering, using the list companies of company names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you did in the previous exercise.\n",
        "\n",
        "\n",
        "# Import normalize\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the movements: normalized_movements\n",
        "normalized_movements = normalize(movements)\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(normalized_movements, method='complete')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(\n",
        "    mergings,\n",
        "    labels=companies,\n",
        "    leaf_rotation=90,\n",
        "    leaf_font_size=6\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIXolY-V571t"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import linkage and dendrogram from scipy.cluster.hierarchy.\n",
        "Perform hierarchical clustering on samples using the linkage() function with the method='single' keyword argument. Assign the result to mergings.\n",
        "Plot a dendrogram of the hierarchical clustering, using the list country_names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you have done earlier.\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='single')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(mergings,\n",
        "           labels=country_names,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6,\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVQUDbKC68uC"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "pandas as pd.\n",
        "fcluster from scipy.cluster.hierarchy.\n",
        "Perform a flat hierarchical clustering by using the fcluster() function on mergings. Specify a maximum height of 6 and the keyword argument criterion='distance'.\n",
        "Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\n",
        "Create a cross-tabulation ct between df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label.\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Use fcluster to extract labels: labels\n",
        "labels = fcluster(mergings, 6, criterion='distance')\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSWlnRCD_9YG"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import TSNE from sklearn.manifold.\n",
        "Create a TSNE instance called model with learning_rate=200.\n",
        "Apply the .fit_transform() method of model to samples. Assign the result to tsne_features.\n",
        "Select the column 0 of tsne_features. Assign the result to xs.\n",
        "Select the column 1 of tsne_features. Assign the result to ys.\n",
        "Make a scatter plot of the t-SNE features xs and ys. To color the points by the grain variety, specify the additional keyword argument c=variety_numbers.\n",
        "\n",
        "\n",
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=200)\n",
        "\n",
        "# Apply fit_transform to samples: tsne_features\n",
        "tsne_features = model.fit_transform(samples)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1st feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot, coloring by variety_numbers\n",
        "plt.scatter(xs, ys, c=variety_numbers)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxf6m7SkDrMG"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import TSNE from sklearn.manifold.\n",
        "Create a TSNE instance called model with learning_rate=50.\n",
        "Apply the .fit_transform() method of model to normalized_movements. Assign the result to tsne_features.\n",
        "Select column 0 and column 1 of tsne_features.\n",
        "Make a scatter plot of the t-SNE features xs and ys. Specify the additional keyword argument alpha=0.5.\n",
        "Code to label each point with its company name has been written for you using plt.annotate(), so just hit 'Submit Answer' to see the visualization!\n",
        "\n",
        "\n",
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=50)\n",
        "\n",
        "# Apply fit_transform to normalized_movements: tsne_features\n",
        "tsne_features = model.fit_transform(normalized_movements)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1th feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(xs, ys, alpha=0.5)\n",
        "\n",
        "# Annotate the points\n",
        "for x, y, company in zip(xs, ys, companies):\n",
        "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ereZv34XEt1T"
      },
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkheqVAiEwQI"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "matplotlib.pyplot as plt.\n",
        "pearsonr from scipy.stats.\n",
        "Assign column 0 of grains to width and column 1 of grains to length.\n",
        "Make a scatter plot with width on the x-axis and length on the y-axis.\n",
        "Use the pearsonr() function to calculate the Pearson correlation of width and length.\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:,0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:,1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ByE56HLFYgo"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import PCA from sklearn.decomposition.\n",
        "Create an instance of PCA called model.\n",
        "Use the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.\n",
        "The subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit 'Submit Answer' to see the result!\n",
        "\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3JwPfnnGvV-"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Make a scatter plot of the grain measurements. This has been done for you.\n",
        "Create a PCA instance called model.\n",
        "Fit the model to the grains data.\n",
        "Extract the coordinates of the mean of the data using the .mean_ attribute of model.\n",
        "Get the first principal component of model using the .components_[0,:] attribute.\n",
        "Plot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1].\n",
        "\n",
        "\n",
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMSa5rh_G9d5"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create an instance of StandardScaler called scaler.\n",
        "Create a PCA instance called pca.\n",
        "Use the make_pipeline() function to create a pipeline chaining scaler and pca.\n",
        "Use the .fit() method of pipeline to fit it to the fish samples samples.\n",
        "Extract the number of components used using the .n_components_ attribute of pca. Place this inside a range() function and store the result as features.\n",
        "Use the plt.bar() function to plot the explained variances, with features on the x-axis and pca.explained_variance_ on the y-axis.\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-39pxw7HLnl"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import PCA from sklearn.decomposition.\n",
        "Create a PCA instance called pca with n_components=2.\n",
        "Use the .fit() method of pca to fit it to the scaled fish measurements scaled_samples.\n",
        "Use the .transform() method of pca to transform the scaled_samples. Assign the result to pca_features.\n",
        "\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance with 2 components: pca\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA instance to the scaled samples\n",
        "pca.fit(scaled_samples)\n",
        "\n",
        "# Transform the scaled samples: pca_features\n",
        "pca_features = pca.transform(scaled_samples)\n",
        "\n",
        "# Print the shape of pca_features\n",
        "print(pca_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VskhdlFsHfUI"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import TfidfVectorizer from sklearn.feature_extraction.text.\n",
        "Create a TfidfVectorizer instance called tfidf.\n",
        "Apply .fit_transform() method of tfidf to documents and assign the result to csr_mat. This is a word-frequency array in csr_matrix format.\n",
        "Inspect csr_mat by calling its .toarray() method and printing the result. This has been done for you.\n",
        "The columns of the array correspond to words. Get the list of words by calling the .get_feature_names() method of tfidf, and assign the result to words.\n",
        "\n",
        "\n",
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer: tfidf\n",
        "tfidf = TfidfVectorizer() \n",
        "\n",
        "# Apply fit_transform to document: csr_mat\n",
        "csr_mat = tfidf.fit_transform(documents)\n",
        "\n",
        "# Print result of toarray() method\n",
        "print(csr_mat.toarray())\n",
        "\n",
        "# Get the words: words\n",
        "words = tfidf.get_feature_names()\n",
        "\n",
        "# Print words\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R8M4x2zHpba"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "TruncatedSVD from sklearn.decomposition.\n",
        "KMeans from sklearn.cluster.\n",
        "make_pipeline from sklearn.pipeline.\n",
        "Create a TruncatedSVD instance called svd with n_components=50.\n",
        "Create a KMeans instance called kmeans with n_clusters=6.\n",
        "Create a pipeline called pipeline consisting of svd and kmeans.\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a TruncatedSVD instance: svd\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "\n",
        "# Create a KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=6)\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(svd, kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bgj8ki5IEcr"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Fit the pipeline to the word-frequency array articles.\n",
        "Predict the cluster labels.\n",
        "Align the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.\n",
        "Use the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.\n",
        "Hit 'Submit Answer' and take a moment to investigate your amazing clustering of Wikipedia pages!\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWTFu2InIMX4"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Fit the pipeline to the word-frequency array articles.\n",
        "Predict the cluster labels.\n",
        "Align the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.\n",
        "Use the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.\n",
        "Hit 'Submit Answer' and take a moment to investigate your amazing clustering of Wikipedia pages!\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi55muz1IeXq"
      },
      "source": [
        "4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pilT3soMIViT"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import NMF from sklearn.decomposition.\n",
        "Create an NMF instance called model with 6 components.\n",
        "Fit the model to the word count data articles.\n",
        "Use the .transform() method of model to transform articles, and assign the result to nmf_features.\n",
        "Print nmf_features to get a first idea what it looks like (.round(2) rounds the entries to 2 decimal places.)\n",
        "\n",
        "\n",
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF instance: model\n",
        "model = NMF(n_components=6)\n",
        "\n",
        "# Fit the model to articles\n",
        "model.fit(articles)\n",
        "\n",
        "# Transform the articles: nmf_features\n",
        "nmf_features = model.transform(articles)\n",
        "\n",
        "# Print the NMF features\n",
        "print(nmf_features.round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ApMmD7LIv83"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import NMF from sklearn.decomposition.\n",
        "Create an NMF instance called model with 6 components.\n",
        "Fit the model to the word count data articles.\n",
        "Use the .transform() method of model to transform articles, and assign the result to nmf_features.\n",
        "Print nmf_features to get a first idea what it looks like (.round(2) rounds the entries to 2 decimal places.)\n",
        "\n",
        "\n",
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF instance: model\n",
        "model = NMF(n_components=6)\n",
        "\n",
        "# Fit the model to articles\n",
        "model.fit(articles)\n",
        "\n",
        "# Transform the articles: nmf_features\n",
        "nmf_features = model.transform(articles)\n",
        "\n",
        "# Print the NMF features\n",
        "print(nmf_features.round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svL9xvfpI3hl"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Create a DataFrame df from nmf_features using pd.DataFrame(). Set the index to titles using index=titles.\n",
        "Use the .loc[] accessor of df to select the row with title 'Anne Hathaway', and print the result. These are the NMF features for the article about the actress Anne Hathaway.\n",
        "Repeat the last step for 'Denzel Washington' (another actor).\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a pandas DataFrame: df\n",
        "df = pd.DataFrame(nmf_features, index=titles)\n",
        "\n",
        "# Print the row for 'Anne Hathaway'\n",
        "print(df.loc['Anne Hathaway'])\n",
        "\n",
        "# Print the row for 'Denzel Washington'\n",
        "print(df.loc['Denzel Washington'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTyH-xN5JKN0"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Create a DataFrame components_df from model.components_, setting columns=words so that columns are labeled by the words.\n",
        "Print components_df.shape to check the dimensions of the DataFrame.\n",
        "Use the .iloc[] accessor on the DataFrame components_df to select row 3. Assign the result to component.\n",
        "Call the .nlargest() method of component, and print the result. This gives the five words with the highest values for that component.\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame: components_df\n",
        "components_df = pd.DataFrame(model.components_, columns=words)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(components_df.shape)\n",
        "\n",
        "# Select row 3: component\n",
        "component = components_df.iloc[3]\n",
        "\n",
        "# Print result of nlargest\n",
        "print(component.nlargest())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCBQsGN5Jljw"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import matplotlib.pyplot as plt.\n",
        "Select row 0 of samples and assign the result to digit. For example, to select column 2 of an array a, you could use a[:,2]. Remember that since samples is a NumPy array, you can't use the .loc[] or iloc[] accessors to select specific rows or columns.\n",
        "Print digit. This has been done for you. Notice that it is a 1D array of 0s and 1s.\n",
        "Use the .reshape() method of digit to get a 2D array with shape (13, 8). Assign the result to bitmap.\n",
        "Print bitmap, and notice that the 1s show the digit 7!\n",
        "Use the plt.imshow() function to display bitmap as an image.\n",
        "\n",
        "\n",
        "# Import pyplot\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Select the 0th row: digit\n",
        "digit = samples[0,:]\n",
        "\n",
        "# Print digit\n",
        "print(digit)\n",
        "\n",
        "# Reshape digit to a 13x8 array: bitmap\n",
        "bitmap = digit.reshape((13, 8))\n",
        "\n",
        "# Print bitmap\n",
        "print(bitmap)\n",
        "\n",
        "# Use plt.imshow to display bitmap\n",
        "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUI6DIfDJxdn"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import NMF from sklearn.decomposition.\n",
        "Create an NMF instance called model with 7 components. (7 is the number of cells in an LED display).\n",
        "Apply the .fit_transform() method of model to samples. Assign the result to features.\n",
        "To each component of the model (accessed via model.components_), apply the show_as_image() function to that component inside the loop.\n",
        "Assign the row 0 of features to digit_features.\n",
        "Print digit_features.\n",
        "\n",
        "\n",
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF model: model\n",
        "model = NMF(n_components=7)\n",
        "\n",
        "# Apply fit_transform to samples: features\n",
        "features = model.fit_transform(samples)\n",
        "\n",
        "# Call show_as_image on each component\n",
        "for component in model.components_:\n",
        "    show_as_image(component)\n",
        "\n",
        "# Select the 0th row of features: digit_features\n",
        "digit_features = features[0,:]\n",
        "\n",
        "# Print digit_features\n",
        "print(digit_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gWnDBb9J-_Q"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import PCA from sklearn.decomposition.\n",
        "Create a PCA instance called model with 7 components.\n",
        "Apply the .fit_transform() method of model to samples. Assign the result to features.\n",
        "To each component of the model (accessed via model.components_), apply the show_as_image() function to that component inside the loop.\n",
        "\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA(n_components=7)\n",
        "\n",
        "# Apply fit_transform to samples: features\n",
        "features = model.fit_transform(samples)\n",
        "\n",
        "# Call show_as_image on each component\n",
        "for component in model.components_:\n",
        "    show_as_image(component)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZzZQm1bKgA9"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import normalize from sklearn.preprocessing.\n",
        "Apply the normalize() function to nmf_features. Store the result as norm_features.\n",
        "Create a DataFrame df from norm_features, using titles as an index.\n",
        "Use the .loc[] accessor of df to select the row of 'Cristiano Ronaldo'. Assign the result to article.\n",
        "Apply the .dot() method of df to article to calculate the cosine similarity of every row with article.\n",
        "Print the result of the .nlargest() method of similarities to display the most similiar articles. This has been done for you, so hit 'Submit Answer' to see the result!\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the NMF features: norm_features\n",
        "norm_features = normalize(nmf_features)\n",
        "\n",
        "# Create a DataFrame: df\n",
        "df = pd.DataFrame(norm_features, index=titles)\n",
        "\n",
        "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
        "article = df.loc['Cristiano Ronaldo']\n",
        "\n",
        "# Compute the dot products: similarities\n",
        "similarities = df.dot(article)\n",
        "\n",
        "# Display those with the largest cosine similarity\n",
        "print(similarities.nlargest())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OzkrepkKqXW"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "NMF from sklearn.decomposition.\n",
        "Normalizer and MaxAbsScaler from sklearn.preprocessing.\n",
        "make_pipeline from sklearn.pipeline.\n",
        "Create an instance of MaxAbsScaler called scaler.\n",
        "Create an NMF instance with 20 components called nmf.\n",
        "Create an instance of Normalizer called normalizer.\n",
        "Create a pipeline called pipeline that chains together scaler, nmf, and normalizer.\n",
        "Apply the .fit_transform() method of pipeline to artists. Assign the result to norm_features.\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a MaxAbsScaler: scaler\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "# Create an NMF model: nmf\n",
        "nmf = NMF(n_components=20)\n",
        "\n",
        "# Create a Normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
        "\n",
        "# Apply fit_transform to artists: norm_features\n",
        "norm_features = pipeline.fit_transform(artists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TErQ8oilKzIk"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "NMF from sklearn.decomposition.\n",
        "Normalizer and MaxAbsScaler from sklearn.preprocessing.\n",
        "make_pipeline from sklearn.pipeline.\n",
        "Create an instance of MaxAbsScaler called scaler.\n",
        "Create an NMF instance with 20 components called nmf.\n",
        "Create an instance of Normalizer called normalizer.\n",
        "Create a pipeline called pipeline that chains together scaler, nmf, and normalizer.\n",
        "Apply the .fit_transform() method of pipeline to artists. Assign the result to norm_features.\n",
        "\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a MaxAbsScaler: scaler\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "# Create an NMF model: nmf\n",
        "nmf = NMF(n_components=20)\n",
        "\n",
        "# Create a Normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
        "\n",
        "# Apply fit_transform to artists: norm_features\n",
        "norm_features = pipeline.fit_transform(artists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU2qDakvK_E1"
      },
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Create a DataFrame df from norm_features, using artist_names as an index.\n",
        "Use the .loc[] accessor of df to select the row of 'Bruce Springsteen'. Assign the result to artist.\n",
        "Apply the .dot() method of df to artist to calculate the dot product of every row with artist. Save the result as similarities.\n",
        "Print the result of the .nlargest() method of similarities to display the artists most similar to 'Bruce Springsteen'.\n",
        "\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame: df\n",
        "df = pd.DataFrame(norm_features, index=artist_names)\n",
        "\n",
        "# Select row of 'Bruce Springsteen': artist\n",
        "artist = df.loc['Bruce Springsteen']\n",
        "\n",
        "# Compute cosine similarities: similarities\n",
        "similarities = df.dot(artist)\n",
        "\n",
        "# Display those with highest cosine similarity\n",
        "print(similarities.nlargest())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-CEfDgLKuK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}