{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlDXd8Xhiq3FstOiUOdzvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Introduction_to_statistics_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9XllHlod3kA"
      },
      "outputs": [],
      "source": [
        "Import numpy with the alias np.\n",
        "Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
        "Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
        "\n",
        "# Import numpy with alias np\n",
        "import numpy as np\n",
        "\n",
        "# Filter for Belgium\n",
        "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
        "\n",
        "# Filter for USA\n",
        "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
        "\n",
        "# Calculate mean and median consumption in Belgium\n",
        "print(np.mean(be_consumption['consumption']))\n",
        "print(np.median(be_consumption['consumption']))\n",
        "\n",
        "\n",
        "Subset food_consumption for rows with data about Belgium and the USA.\n",
        "Group the subsetted data by country and select only the consumption column.\n",
        "Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
        "\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Import matplotlib.pyplot with the alias plt.\n",
        "Subset food_consumption to get the rows where food_category is 'rice'.\n",
        "Create a histogram of co2_emission for rice and show the plot.\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Histogram of co2_emission for rice and show plot\n",
        "rice_consumption['co2_emission'].hist()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Use .agg() to calculate the mean and median of co2_emission for rice.\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Calculate mean and median of co2_emission with .agg()\n",
        "print(rice_consumption['co2_emission'].agg([np.mean, np.median]))"
      ],
      "metadata": {
        "id": "Wczbzg4enj_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "Calculate the quartiles of the co2_emission column of food_consumption.\n",
        "\n",
        "# Calculate the quartiles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], [0, 0.25, 0.5, 0.75, 1]))\n",
        "\n",
        "\n",
        "2. Calculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column of food_consumption.\n",
        "\n",
        "# Calculate the quintiles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1]))\n",
        "\n",
        "\n",
        "3. Calculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles).\n",
        "\n",
        "# Calculate the deciles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 11)))\n"
      ],
      "metadata": {
        "id": "83EPfmgMpraP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating.\n",
        "Import matplotlib.pyplot with alias plt.\n",
        "Create a histogram of co2_emission for the beef food_category and show the plot.\n",
        "Create a histogram of co2_emission for the eggs food_category and show the plot.\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dZ9zsvuoq_O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Calculate the total co2_emission per country by grouping by country and taking the sum of co2_emission. Store the resulting DataFrame as emissions_by_country\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "print(emissions_by_country)\n",
        "\n",
        "\n",
        "2. Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Compute the first and third quartiles of emissions_by_country and store these as q1 and q3.\n",
        "Calculate the interquartile range of emissions_by_country and store it as iqr.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quartiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Calculate the lower and upper cutoffs for outliers of emissions_by_country, and store these as lower and upper.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Subset emissions_by_country to get countries with a total emission greater than the upper cutoff or a total emission less than the lower cutoff.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "# Subset emissions_by_country to find outliers\n",
        "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
        "print(outliers)\n"
      ],
      "metadata": {
        "id": "S6A03E9t0qAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "Import numpy with the alias np.\n",
        "Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
        "Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
        "\n",
        "# Import numpy with alias np\n",
        "import numpy as np\n",
        "\n",
        "# Filter for Belgium\n",
        "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
        "\n",
        "# Filter for USA\n",
        "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
        "\n",
        "# Calculate mean and median consumption in Belgium\n",
        "print(np.mean(be_consumption['consumption']))\n",
        "print(np.median(be_consumption['consumption']))\n",
        "\n",
        "\n",
        "\n",
        "Subset food_consumption for rows with data about Belgium and the USA.\n",
        "Group the subsetted data by country and select only the consumption column.\n",
        "Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n"
      ],
      "metadata": {
        "id": "h7WTLrzc1Dqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "0 XP\n",
        "Import matplotlib.pyplot with the alias plt.\n",
        "Subset food_consumption to get the rows where food_category is 'rice'.\n",
        "Create a histogram of co2_emission for rice and show the plot.\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Histogram of co2_emission for rice and show plot\n",
        "rice_consumption['co2_emission'].hist()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GrCfxaibjSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating.\n",
        "Import matplotlib.pyplot with alias plt.\n",
        "Create a histogram of co2_emission for the beef food_category and show the plot.\n",
        "Create a histogram of co2_emission for the eggs food_category and show the plot.\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yZ2mFD8ejyJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "oJsG2quyvHHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Count the number of deals Amir worked on for each product type and store in counts.\n",
        "\n",
        "# Count the deals for each product\n",
        "counts = amir_deals['product'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Calculate the probability of selecting a deal for the different product types by dividing the counts by the total number of deals Amir worked on. Save this as probs.\n",
        "\n",
        "\n",
        "# Count the deals for each product\n",
        "counts = amir_deals['product'].value_counts()\n",
        "\n",
        "# Calculate probability of picking a deal with each product\n",
        "probs = counts / amir_deals.shape[0]\n",
        "print(probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "s0hHdzg-vGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Set the random seed to 24.\n",
        "Take a sample of 5 deals without replacement and store them as sample_without_replacement.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(24)\n",
        "\n",
        "# Sample 5 deals without replacement\n",
        "sample_without_replacement = amir_deals.sample(5)\n",
        "print(sample_without_replacement)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Take a sample of 5 deals with replacement and save as sample_with_replacement.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(24)\n",
        "\n",
        "# Sample 5 deals with replacement\n",
        "sample_with_replacement = amir_deals.sample(5, replace=True)\n",
        "print(sample_with_replacement)\n"
      ],
      "metadata": {
        "id": "l-dXQhIP_OMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "Create a histogram of the group_size column of restaurant_groups, setting bins to [2, 3, 4, 5, 6]. Remember to show the plot.\n",
        "\n",
        "# Create a histogram of restaurant_groups and show plot\n",
        "restaurant_groups['group_size'].hist(bins=np.linspace(2,6,5))\n",
        "plt.show()\n",
        "\n",
        "Count the number of each group_size in restaurant_groups, then divide by the number of rows in restaurant_groups to calculate the probability of randomly selecting a group of each size. Save as size_dist.\n",
        "Reset the index of size_dist.\n",
        "Rename the columns of size_dist to group_size and prob.\n",
        "\n",
        "\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "print(size_dist)\n",
        "\n",
        "\n",
        "Calculate the expected value of the size_distribution, which represents the expected group size, by multiplying the group_size by the prob and taking the sum.\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "# Expected value\n",
        "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
        "print(expected_value)\n",
        "\n",
        "\n",
        "Calculate the probability of randomly picking a group of 4 or more people by subsetting for groups of size 4 or more and summing the probabilities of selecting those groups.\n",
        "\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "# Expected value\n",
        "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
        "\n",
        "# Subset groups of size 4 or more\n",
        "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
        "\n",
        "# Sum the probabilities of groups_4_or_more\n",
        "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
        "print(prob_4_or_more)"
      ],
      "metadata": {
        "id": "_JBy919tEx5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "To model how long Amir will wait for a back-up using a continuous uniform distribution, save his lowest possible wait time as min_time and his longest possible wait time as max_time. Remember that back-ups happen every 30 minutes.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "\n",
        "Import uniform from scipy.stats and calculate the probability that Amir has to wait less than 5 minutes, and store in a variable called prob_less_than_5.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting less than 5 mins\n",
        "prob_less_than_5 = uniform.cdf(5, min_time, max_time)\n",
        "print(prob_less_than_5)\n",
        "\n",
        "Calculate the probability that Amir has to wait more than 5 minutes, and store in a variable called prob_greater_than_5.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting more than 5 mins\n",
        "prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\n",
        "print(prob_greater_than_5)\n",
        "\n",
        "\n",
        "Calculate the probability that Amir has to wait between 10 and 20 minutes, and store in a variable called prob_between_10_and_20.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting 10-20 mins\n",
        "prob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\n",
        "print(prob_between_10_and_20)\n"
      ],
      "metadata": {
        "id": "E4tGTSWe8OEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G0Yvewva_nGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-lblgbS_nCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APaxcaxi8OBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVLXlpIz8N-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGY0A2rV8N4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}