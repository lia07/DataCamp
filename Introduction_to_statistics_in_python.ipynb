{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX8YGeEOu+vEtX/71I2Vgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Introduction_to_statistics_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9XllHlod3kA"
      },
      "outputs": [],
      "source": [
        "Import numpy with the alias np.\n",
        "Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
        "Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
        "\n",
        "# Import numpy with alias np\n",
        "import numpy as np\n",
        "\n",
        "# Filter for Belgium\n",
        "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
        "\n",
        "# Filter for USA\n",
        "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
        "\n",
        "# Calculate mean and median consumption in Belgium\n",
        "print(np.mean(be_consumption['consumption']))\n",
        "print(np.median(be_consumption['consumption']))\n",
        "\n",
        "\n",
        "Subset food_consumption for rows with data about Belgium and the USA.\n",
        "Group the subsetted data by country and select only the consumption column.\n",
        "Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
        "\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Import matplotlib.pyplot with the alias plt.\n",
        "Subset food_consumption to get the rows where food_category is 'rice'.\n",
        "Create a histogram of co2_emission for rice and show the plot.\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Histogram of co2_emission for rice and show plot\n",
        "rice_consumption['co2_emission'].hist()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Use .agg() to calculate the mean and median of co2_emission for rice.\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Calculate mean and median of co2_emission with .agg()\n",
        "print(rice_consumption['co2_emission'].agg([np.mean, np.median]))"
      ],
      "metadata": {
        "id": "Wczbzg4enj_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "Calculate the quartiles of the co2_emission column of food_consumption.\n",
        "\n",
        "# Calculate the quartiles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], [0, 0.25, 0.5, 0.75, 1]))\n",
        "\n",
        "\n",
        "2. Calculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column of food_consumption.\n",
        "\n",
        "# Calculate the quintiles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1]))\n",
        "\n",
        "\n",
        "3. Calculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles).\n",
        "\n",
        "# Calculate the deciles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 11)))\n"
      ],
      "metadata": {
        "id": "83EPfmgMpraP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating.\n",
        "Import matplotlib.pyplot with alias plt.\n",
        "Create a histogram of co2_emission for the beef food_category and show the plot.\n",
        "Create a histogram of co2_emission for the eggs food_category and show the plot.\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dZ9zsvuoq_O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Calculate the total co2_emission per country by grouping by country and taking the sum of co2_emission. Store the resulting DataFrame as emissions_by_country\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "print(emissions_by_country)\n",
        "\n",
        "\n",
        "2. Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Compute the first and third quartiles of emissions_by_country and store these as q1 and q3.\n",
        "Calculate the interquartile range of emissions_by_country and store it as iqr.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quartiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Calculate the lower and upper cutoffs for outliers of emissions_by_country, and store these as lower and upper.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Subset emissions_by_country to get countries with a total emission greater than the upper cutoff or a total emission less than the lower cutoff.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "# Subset emissions_by_country to find outliers\n",
        "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
        "print(outliers)\n"
      ],
      "metadata": {
        "id": "S6A03E9t0qAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "Import numpy with the alias np.\n",
        "Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
        "Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
        "\n",
        "# Import numpy with alias np\n",
        "import numpy as np\n",
        "\n",
        "# Filter for Belgium\n",
        "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
        "\n",
        "# Filter for USA\n",
        "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
        "\n",
        "# Calculate mean and median consumption in Belgium\n",
        "print(np.mean(be_consumption['consumption']))\n",
        "print(np.median(be_consumption['consumption']))\n",
        "\n",
        "\n",
        "\n",
        "Subset food_consumption for rows with data about Belgium and the USA.\n",
        "Group the subsetted data by country and select only the consumption column.\n",
        "Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n"
      ],
      "metadata": {
        "id": "h7WTLrzc1Dqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "0 XP\n",
        "Import matplotlib.pyplot with the alias plt.\n",
        "Subset food_consumption to get the rows where food_category is 'rice'.\n",
        "Create a histogram of co2_emission for rice and show the plot.\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Histogram of co2_emission for rice and show plot\n",
        "rice_consumption['co2_emission'].hist()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GrCfxaibjSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating.\n",
        "Import matplotlib.pyplot with alias plt.\n",
        "Create a histogram of co2_emission for the beef food_category and show the plot.\n",
        "Create a histogram of co2_emission for the eggs food_category and show the plot.\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yZ2mFD8ejyJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "oJsG2quyvHHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Count the number of deals Amir worked on for each product type and store in counts.\n",
        "\n",
        "# Count the deals for each product\n",
        "counts = amir_deals['product'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Calculate the probability of selecting a deal for the different product types by dividing the counts by the total number of deals Amir worked on. Save this as probs.\n",
        "\n",
        "\n",
        "# Count the deals for each product\n",
        "counts = amir_deals['product'].value_counts()\n",
        "\n",
        "# Calculate probability of picking a deal with each product\n",
        "probs = counts / amir_deals.shape[0]\n",
        "print(probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "s0hHdzg-vGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Set the random seed to 24.\n",
        "Take a sample of 5 deals without replacement and store them as sample_without_replacement.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(24)\n",
        "\n",
        "# Sample 5 deals without replacement\n",
        "sample_without_replacement = amir_deals.sample(5)\n",
        "print(sample_without_replacement)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Take a sample of 5 deals with replacement and save as sample_with_replacement.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(24)\n",
        "\n",
        "# Sample 5 deals with replacement\n",
        "sample_with_replacement = amir_deals.sample(5, replace=True)\n",
        "print(sample_with_replacement)\n"
      ],
      "metadata": {
        "id": "l-dXQhIP_OMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "Create a histogram of the group_size column of restaurant_groups, setting bins to [2, 3, 4, 5, 6]. Remember to show the plot.\n",
        "\n",
        "# Create a histogram of restaurant_groups and show plot\n",
        "restaurant_groups['group_size'].hist(bins=np.linspace(2,6,5))\n",
        "plt.show()\n",
        "\n",
        "Count the number of each group_size in restaurant_groups, then divide by the number of rows in restaurant_groups to calculate the probability of randomly selecting a group of each size. Save as size_dist.\n",
        "Reset the index of size_dist.\n",
        "Rename the columns of size_dist to group_size and prob.\n",
        "\n",
        "\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "print(size_dist)\n",
        "\n",
        "\n",
        "Calculate the expected value of the size_distribution, which represents the expected group size, by multiplying the group_size by the prob and taking the sum.\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "# Expected value\n",
        "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
        "print(expected_value)\n",
        "\n",
        "\n",
        "Calculate the probability of randomly picking a group of 4 or more people by subsetting for groups of size 4 or more and summing the probabilities of selecting those groups.\n",
        "\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "# Expected value\n",
        "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
        "\n",
        "# Subset groups of size 4 or more\n",
        "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
        "\n",
        "# Sum the probabilities of groups_4_or_more\n",
        "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
        "print(prob_4_or_more)"
      ],
      "metadata": {
        "id": "_JBy919tEx5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "To model how long Amir will wait for a back-up using a continuous uniform distribution, save his lowest possible wait time as min_time and his longest possible wait time as max_time. Remember that back-ups happen every 30 minutes.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "\n",
        "Import uniform from scipy.stats and calculate the probability that Amir has to wait less than 5 minutes, and store in a variable called prob_less_than_5.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting less than 5 mins\n",
        "prob_less_than_5 = uniform.cdf(5, min_time, max_time)\n",
        "print(prob_less_than_5)\n",
        "\n",
        "Calculate the probability that Amir has to wait more than 5 minutes, and store in a variable called prob_greater_than_5.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting more than 5 mins\n",
        "prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\n",
        "print(prob_greater_than_5)\n",
        "\n",
        "\n",
        "Calculate the probability that Amir has to wait between 10 and 20 minutes, and store in a variable called prob_between_10_and_20.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting 10-20 mins\n",
        "prob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\n",
        "print(prob_between_10_and_20)\n"
      ],
      "metadata": {
        "id": "E4tGTSWe8OEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Set the random seed to 334\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Import uniform from scipy.stats.\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Generate 1000 wait times from the continuous uniform distribution that models Amir's wait time. Save this as wait_times.\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Generate 1000 wait times between 0 and 30 mins\n",
        "wait_times = uniform.rvs(0, 30, size=1000)\n",
        "\n",
        "print(wait_times)\n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Create a histogram of the simulated wait times and show the plot.\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Generate 1000 wait times between 0 and 30 mins\n",
        "wait_times = uniform.rvs(0, 30, size=1000)\n",
        "\n",
        "# Create a histogram of simulated times and show plot\n",
        "plt.hist(wait_times)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G0Yvewva_nGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nstructions 1/4\n",
        "25 XP\n",
        "1\n",
        "Import binom from scipy.stats and set the random seed to 10.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "\n",
        "Simulate 1 deal worked on by Amir, who wins 30% of the deals he works on.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate a single deal\n",
        "print(binom.rvs(1, 0.3, size=1))\n",
        "\n",
        "Simulate a typical week of Amir's deals, or one week of 3 deals.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate 1 week of 3 deals\n",
        "print(binom.rvs(3, 0.3, size=1))\n",
        "\n",
        "\n",
        "Simulate a year's worth of Amir's deals, or 52 weeks of 3 deals each, and store in deals.\n",
        "Print the mean number of deals he won per week.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate 52 weeks of 3 deals\n",
        "deals = binom.rvs(3, 0.3, size=52)\n",
        "\n",
        "# Print mean deals won per week\n",
        "print(np.mean(deals))\n"
      ],
      "metadata": {
        "id": "B-lblgbS_nCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "What's the probability that Amir closes all 3 deals in a week? Save this as prob_3.\n",
        "\n",
        "# Probability of closing 3 out of 3 deals\n",
        "prob_3 = binom.pmf(3, 3, 0.3)\n",
        "\n",
        "print(prob_3)\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "What's the probability that Amir closes 1 or fewer deals in a week? Save this as prob_less_than_or_equal_1.\n",
        "\n",
        "# Probability of closing <= 1 deal out of 3 deals\n",
        "prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n",
        "\n",
        "print(prob_less_than_or_equal_1)\n",
        "\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "What's the probability that Amir closes more than 1 deal? Save this as prob_greater_than_1.\n",
        "\n",
        "# Probability of closing > 1 deal out of 3 deals\n",
        "prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)\n",
        "\n",
        "print(prob_greater_than_1)\n"
      ],
      "metadata": {
        "id": "APaxcaxi8OBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.\n",
        "Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate drops to 25%.\n",
        "Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate rises to 35%.\n",
        "\n",
        "# Expected number won with 30% win rate\n",
        "won_30pct = 3 * 0.3\n",
        "print(won_30pct)\n",
        "\n",
        "# Expected number won with 25% win rate\n",
        "won_25pct = 3 * 0.25\n",
        "print(won_25pct)\n",
        "\n",
        "# Expected number won with 35% win rate\n",
        "won_35pct = 3 * 0.35\n",
        "print(won_35pct)"
      ],
      "metadata": {
        "id": "zVLXlpIz8N-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "vcSGp_9kqvWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Create a histogram with 10 bins to visualize the distribution of the amount. Show the plot.\n",
        "\n",
        "\n",
        "# Histogram of amount with 10 bins and show plot\n",
        "amir_deals['amount'].hist(bins=10)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QGY0A2rV8N4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "1 XP\n",
        "1\n",
        "What's the probability of Amir closing a deal worth less than $7500?\n",
        "\n",
        "# Probability of deal < 7500\n",
        "prob_less_7500 = norm.cdf(7500, 5000, 2000)\n",
        "\n",
        "print(prob_less_7500)\n",
        "\n",
        "\n",
        "\n",
        "What's the probability of Amir closing a deal worth more than $1000?\n",
        "\n",
        "\n",
        "# Probability of deal > 1000\n",
        "prob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)\n",
        "\n",
        "print(prob_over_1000)\n",
        "\n",
        "\n",
        "What's the probability of Amir closing a deal worth between $3000 and $7000?\n",
        "\n",
        "# Probability of deal between 3000 and 7000\n",
        "prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\n",
        "\n",
        "print(prob_3000_to_7000)\n",
        "\n",
        "\n",
        "What amount will 25% of Amir's sales be less than?\n",
        "\n",
        "# Calculate amount that 25% of deals will be less than\n",
        "pct_25 = norm.ppf(0.25, 5000, 2000)\n",
        "\n",
        "print(pct_25)"
      ],
      "metadata": {
        "id": "64m39ZmpwJkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Currently, Amir's average sale amount is $5000. Calculate what his new average amount will be if it increases by 20% and store this in new_mean.\n",
        "Amir's current standard deviation is $2000. Calculate what his new standard deviation will be if it increases by 30% and store this in new_sd.\n",
        "Create a variable called new_sales, which contains 36 simulated amounts from a normal distribution with a mean of new_mean and a standard deviation of new_sd.\n",
        "Plot the distribution of the new_sales amounts using a histogram and show the plot.\n",
        "\n",
        "\n",
        "# Calculate new average amount\n",
        "new_mean = 5000 * 1.2\n",
        "\n",
        "# Calculate new standard deviation\n",
        "new_sd = 2000 * 1.3\n",
        "\n",
        "# Simulate 36 new sales\n",
        "new_sales = norm.rvs(new_mean, new_sd, size=36)\n",
        "\n",
        "# Create histogram and show\n",
        "plt.hist(new_sales)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "imDueSoQwJgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Set the random seed to 321.\n",
        "Take 30 samples (with replacement) of size 20 from all_deals['num_users'] and take the mean of each sample. Store the sample means in sample_means.\n",
        "Print the mean of sample_means.\n",
        "Print the mean of the num_users column of amir_deals.\n",
        "\n",
        "# Set seed to 321\n",
        "np.random.seed(321)\n",
        "\n",
        "sample_means = []\n",
        "# Loop 30 times to take 30 means\n",
        "for i in range(30):\n",
        "  # Take sample of size 20 from num_users col of all_deals with replacement\n",
        "  cur_sample = all_deals['num_users'].sample(20, replace=True)\n",
        "  # Take mean of cur_sample\n",
        "  cur_mean = np.mean(cur_sample)\n",
        "  # Append cur_mean to sample_means\n",
        "  sample_means.append(cur_mean)\n",
        "\n",
        "# Print mean of sample_means\n",
        "print(np.mean(sample_means))\n",
        "\n",
        "# Print mean of num_users in amir_deals\n",
        "print(np.mean(amir_deals['num_users']))\n"
      ],
      "metadata": {
        "id": "9IjuoItzzrpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fFwAfEHUzrWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Import poisson from scipy.stats and calculate the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4.\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of 5 responses\n",
        "prob_5 = poisson.pmf(5, 4)\n",
        "\n",
        "print(prob_5)\n",
        "\n",
        "\n",
        "Amir's coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of 5 responses\n",
        "prob_coworker = poisson.pmf(5, 5.5)\n",
        "\n",
        "print(prob_coworker)\n",
        "\n",
        "\n",
        "What's the probability that Amir responds to 2 or fewer leads in a day?\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of 2 or fewer responses\n",
        "prob_2_or_less = poisson.cdf(2, 4)\n",
        "\n",
        "print(prob_2_or_less)\n",
        "\n",
        "What's the probability that Amir responds to more than 10 leads in a day?\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of > 10 responses\n",
        "prob_over_10 = 1 - poisson.cdf(10, 4)\n",
        "\n",
        "print(prob_over_10)"
      ],
      "metadata": {
        "id": "B4ClElcD6kGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "1 XP\n",
        "Import expon from scipy.stats. What's the probability it takes Amir less than an hour to respond to a lead?\n",
        "\n",
        "\n",
        "# Import expon from scipy.stats\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Print probability response takes < 1 hour\n",
        "print(expon.cdf(1, scale=2.5))\n",
        "\n",
        "\n",
        "What's the probability it takes Amir more than 4 hours to respond to a lead?\n",
        "\n",
        "# Import expon from scipy.stats\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Print probability response takes > 4 hours\n",
        "print(1 - expon.cdf(4, scale=2.5))\n",
        "\n",
        "\n",
        "What's the probability it takes Amir 3-4 hours to respond to a lead?\n",
        "\n",
        "# Import expon from scipy.stats\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Print probability response takes 3-4 hours\n",
        "print(expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5))\n"
      ],
      "metadata": {
        "id": "47BdGCxF6kC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "bp1uKqQ1EjVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Create a scatterplot of happiness_score vs. life_exp (without a trendline) using seaborn.\n",
        "Show the plot.\n",
        "\n",
        "# Create a scatterplot of happiness_score vs. life_exp and show\n",
        "sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Create a scatterplot of happiness_score vs. life_exp with a linear trendline using seaborn, setting ci to None.\n",
        "Show the plot.\n",
        "\n",
        "# Create scatterplot of happiness_score vs life_exp with trendline\n",
        "sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "g_QbEzEvEjzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Create a seaborn scatterplot (without a trendline) showing the relationship between gdp_per_cap (on the x-axis) and life_exp (on the y-axis).\n",
        "Show the plot\n",
        "\n",
        "# Scatterplot of gdp_per_cap and life_exp\n",
        "sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Calculate the correlation between gdp_per_cap and life_exp and store as cor.\n",
        "\n",
        "\n",
        "# Scatterplot of gdp_per_cap and life_exp\n",
        "sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "  \n",
        "# Correlation between gdp_per_cap and life_exp\n",
        "cor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n",
        "\n",
        "print(cor)\n",
        "\n"
      ],
      "metadata": {
        "id": "_e91gANbwJcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a scatterplot of happiness_score versus gdp_per_cap and calculate the correlation between them.\n",
        "\n",
        "# Scatterplot of happiness_score vs. gdp_per_cap\n",
        "sns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "cor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
        "print(cor)\n",
        "\n",
        "\n",
        "Add a new column to world_happiness called log_gdp_per_cap that contains the log of gdp_per_cap.\n",
        "Create a seaborn scatterplot of log_gdp_per_cap and happiness_score and calculate the correlation between them.\n",
        "\n",
        "# Create log_gdp_per_cap column\n",
        "world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n",
        "\n",
        "# Scatterplot of log_gdp_per_cap and happiness_score\n",
        "sns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
        "print(cor)\n"
      ],
      "metadata": {
        "id": "fg6yx6T3wJYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "1\n",
        "2\n",
        "Create a seaborn scatterplot showing the relationship between grams_sugar_per_day (on the x-axis) and happiness_score (on the y-axis).\n",
        "Calculate the correlation between grams_sugar_per_day and happiness_score.\n",
        "\n",
        "\n",
        "# Scatterplot of grams_sugar_per_day and happiness_score\n",
        "sns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\n",
        "plt.show()\n",
        "\n",
        "# Correlation between grams_sugar_per_day and happiness_score\n",
        "cor = world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\n",
        "print(cor)\n",
        "\n"
      ],
      "metadata": {
        "id": "vFZ0ugdib1Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONT**"
      ],
      "metadata": {
        "id": "TkKnLiM5B3K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 70 rows using simple random sampling and set the seed\n",
        "attrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_samp)"
      ],
      "metadata": {
        "id": "bO4Mhj1Db1O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Set the sample size to 70.\n",
        "Calculate the population size from attrition_pop.\n",
        "Calculate the interval between the rows to be sampled.\n",
        "\n",
        "# Set the sample size to 70\n",
        "sample_size = 70\n",
        "\n",
        "# Calculate the population size from attrition_pop\n",
        "pop_size = len(attrition_pop)\n",
        "\n",
        "# Calculate the interval\n",
        "interval = pop_size // sample_size\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Systematically sample attrition_pop to get the rows of the population at each interval, starting at 0; assign the rows to attrition_sys_samp.\n",
        "\n",
        "# Set the sample size to 70\n",
        "sample_size = 70\n",
        "\n",
        "# Calculate the population size from attrition_pop\n",
        "pop_size = len(attrition_pop)\n",
        "\n",
        "# Calculate the interval\n",
        "interval = pop_size // sample_size\n",
        "\n",
        "# Systematically sample 70 rows\n",
        "attrition_sys_samp = attrition_pop.iloc[::interval]\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_sys_samp)"
      ],
      "metadata": {
        "id": "LkvubHPtB2nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "1 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Add an index column to attrition_pop, assigning the result to attrition_pop_id.\n",
        "Create a scatter plot of YearsAtCompany versus index for attrition_pop_id using pandas .plot().\n",
        "\n",
        "# Add an index column to attrition_pop\n",
        "attrition_pop_id = attrition_pop.reset_index()\n",
        "\n",
        "# Plot YearsAtCompany vs. index for attrition_pop_id\n",
        "attrition_pop_id.plot(x=\"index\", y=\"YearsAtCompany\", kind=\"scatter\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Randomly shuffle the rows of attrition_pop.\n",
        "Reset the row indexes, and add an index column to attrition_pop.\n",
        "Repeat the scatter plot of YearsAtCompany versus index, this time using attrition_shuffled.\n",
        "\n",
        "\n",
        "# Shuffle the rows of attrition_pop\n",
        "attrition_shuffled = attrition_pop.sample(frac=1)\n",
        "\n",
        "# Reset the row indexes and create an index column\n",
        "attrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n",
        "\n",
        "# Plot YearsAtCompany vs. index for attrition_shuffled\n",
        "attrition_shuffled.plot(x=\"index\", y=\"YearsAtCompany\", kind=\"scatter\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rX-s3qN-B2j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Get the proportion of employees by Education level from attrition_pop.\n",
        "\n",
        "\n",
        "# Proportion of employees by Education level\n",
        "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_pop\n",
        "print(education_counts_pop)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Use proportional stratified sampling on attrition_pop to sample 40% of each Education group, setting the seed to 2022.\n",
        "\n",
        "# Proportion of employees by Education level\n",
        "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_pop\n",
        "print(education_counts_pop)\n",
        "\n",
        "# Proportional stratified sampling for 40% of each Education group\n",
        "attrition_strat = attrition_pop.groupby('Education')\\\n",
        "\t.sample(frac=0.4, random_state=2022)\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_strat)\n",
        "\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Get the proportion of employees by Education level from attrition_strat.\n",
        "\n",
        "# Proportion of employees by Education level\n",
        "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_pop\n",
        "print(education_counts_pop)\n",
        "\n",
        "# Proportional stratified sampling for 40% of each Education group\n",
        "attrition_strat = attrition_pop.groupby('Education')\\\n",
        "\t.sample(frac=0.4, random_state=2022)\n",
        "\n",
        "# Calculate the Education level proportions from attrition_strat\n",
        "education_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_strat\n",
        "print(education_counts_strat)"
      ],
      "metadata": {
        "id": "YJzHnGJNB2gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Use equal counts stratified sampling on attrition_pop to get 30 employees from each Education group, setting the seed to 2022.\n",
        "\n",
        "\n",
        "# Get 30 employees from each Education group\n",
        "attrition_eq = attrition_pop.groupby('Education')\\\n",
        "\t.sample(n=30, random_state=2022)      \n",
        "         \n",
        "# Print the sample\n",
        "print(attrition_eq)\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Get the proportion of employees by Education level from attrition_eq.\n",
        "\n",
        "# Get 30 employees from each Education group\n",
        "attrition_eq = attrition_pop.groupby('Education')\\\n",
        "\t.sample(n=30, random_state=2022)      \n",
        "\n",
        "# Get the proportions from attrition_eq\n",
        "education_counts_eq = attrition_eq['Education'].value_counts(normalize=True) \n",
        "\n",
        "# Print the results\n",
        "print(education_counts_eq)\n"
      ],
      "metadata": {
        "id": "1HvCQ_LMB2O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJeOFjtYB2L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FfvqYzIiB1-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKlc_JZkB16X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}