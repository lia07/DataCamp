{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjbTkxtsImkYMc88P4H1qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Introduction_to_statistics_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9XllHlod3kA"
      },
      "outputs": [],
      "source": [
        "Import numpy with the alias np.\n",
        "Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
        "Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
        "\n",
        "# Import numpy with alias np\n",
        "import numpy as np\n",
        "\n",
        "# Filter for Belgium\n",
        "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
        "\n",
        "# Filter for USA\n",
        "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
        "\n",
        "# Calculate mean and median consumption in Belgium\n",
        "print(np.mean(be_consumption['consumption']))\n",
        "print(np.median(be_consumption['consumption']))\n",
        "\n",
        "\n",
        "Subset food_consumption for rows with data about Belgium and the USA.\n",
        "Group the subsetted data by country and select only the consumption column.\n",
        "Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
        "\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Import matplotlib.pyplot with the alias plt.\n",
        "Subset food_consumption to get the rows where food_category is 'rice'.\n",
        "Create a histogram of co2_emission for rice and show the plot.\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Histogram of co2_emission for rice and show plot\n",
        "rice_consumption['co2_emission'].hist()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Use .agg() to calculate the mean and median of co2_emission for rice.\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Calculate mean and median of co2_emission with .agg()\n",
        "print(rice_consumption['co2_emission'].agg([np.mean, np.median]))"
      ],
      "metadata": {
        "id": "Wczbzg4enj_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "Calculate the quartiles of the co2_emission column of food_consumption.\n",
        "\n",
        "# Calculate the quartiles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], [0, 0.25, 0.5, 0.75, 1]))\n",
        "\n",
        "\n",
        "2. Calculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column of food_consumption.\n",
        "\n",
        "# Calculate the quintiles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1]))\n",
        "\n",
        "\n",
        "3. Calculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles).\n",
        "\n",
        "# Calculate the deciles of co2_emission\n",
        "print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 11)))\n"
      ],
      "metadata": {
        "id": "83EPfmgMpraP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating.\n",
        "Import matplotlib.pyplot with alias plt.\n",
        "Create a histogram of co2_emission for the beef food_category and show the plot.\n",
        "Create a histogram of co2_emission for the eggs food_category and show the plot.\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dZ9zsvuoq_O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Calculate the total co2_emission per country by grouping by country and taking the sum of co2_emission. Store the resulting DataFrame as emissions_by_country\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "print(emissions_by_country)\n",
        "\n",
        "\n",
        "2. Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Compute the first and third quartiles of emissions_by_country and store these as q1 and q3.\n",
        "Calculate the interquartile range of emissions_by_country and store it as iqr.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quartiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Calculate the lower and upper cutoffs for outliers of emissions_by_country, and store these as lower and upper.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Subset emissions_by_country to get countries with a total emission greater than the upper cutoff or a total emission less than the lower cutoff.\n",
        "\n",
        "# Calculate total co2_emission per country: emissions_by_country\n",
        "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
        "\n",
        "# Compute the first and third quantiles and IQR of emissions_by_country\n",
        "q1 = np.quantile(emissions_by_country, 0.25)\n",
        "q3 = np.quantile(emissions_by_country, 0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper cutoffs for outliers\n",
        "lower = q1 - 1.5 * iqr\n",
        "upper = q3 + 1.5 * iqr\n",
        "\n",
        "# Subset emissions_by_country to find outliers\n",
        "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n",
        "print(outliers)\n"
      ],
      "metadata": {
        "id": "S6A03E9t0qAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "Import numpy with the alias np.\n",
        "Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
        "Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
        "\n",
        "# Import numpy with alias np\n",
        "import numpy as np\n",
        "\n",
        "# Filter for Belgium\n",
        "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
        "\n",
        "# Filter for USA\n",
        "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
        "\n",
        "# Calculate mean and median consumption in Belgium\n",
        "print(np.mean(be_consumption['consumption']))\n",
        "print(np.median(be_consumption['consumption']))\n",
        "\n",
        "\n",
        "\n",
        "Subset food_consumption for rows with data about Belgium and the USA.\n",
        "Group the subsetted data by country and select only the consumption column.\n",
        "Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
        "\n",
        "# Import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# Subset for Belgium and USA only\n",
        "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
        "\n",
        "# Group by country, select consumption column, and compute mean and median\n",
        "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))\n"
      ],
      "metadata": {
        "id": "h7WTLrzc1Dqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "0 XP\n",
        "Import matplotlib.pyplot with the alias plt.\n",
        "Subset food_consumption to get the rows where food_category is 'rice'.\n",
        "Create a histogram of co2_emission for rice and show the plot.\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Subset for food_category equals rice\n",
        "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
        "\n",
        "# Histogram of co2_emission for rice and show plot\n",
        "rice_consumption['co2_emission'].hist()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GrCfxaibjSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating.\n",
        "Import matplotlib.pyplot with alias plt.\n",
        "Create a histogram of co2_emission for the beef food_category and show the plot.\n",
        "Create a histogram of co2_emission for the eggs food_category and show the plot.\n",
        "\n",
        "# Print variance and sd of co2_emission for each food_category\n",
        "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
        "\n",
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'beef'\n",
        "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Create histogram of co2_emission for food_category 'eggs'\n",
        "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yZ2mFD8ejyJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "oJsG2quyvHHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Count the number of deals Amir worked on for each product type and store in counts.\n",
        "\n",
        "# Count the deals for each product\n",
        "counts = amir_deals['product'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Calculate the probability of selecting a deal for the different product types by dividing the counts by the total number of deals Amir worked on. Save this as probs.\n",
        "\n",
        "\n",
        "# Count the deals for each product\n",
        "counts = amir_deals['product'].value_counts()\n",
        "\n",
        "# Calculate probability of picking a deal with each product\n",
        "probs = counts / amir_deals.shape[0]\n",
        "print(probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "s0hHdzg-vGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Set the random seed to 24.\n",
        "Take a sample of 5 deals without replacement and store them as sample_without_replacement.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(24)\n",
        "\n",
        "# Sample 5 deals without replacement\n",
        "sample_without_replacement = amir_deals.sample(5)\n",
        "print(sample_without_replacement)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Take a sample of 5 deals with replacement and save as sample_with_replacement.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(24)\n",
        "\n",
        "# Sample 5 deals with replacement\n",
        "sample_with_replacement = amir_deals.sample(5, replace=True)\n",
        "print(sample_with_replacement)\n"
      ],
      "metadata": {
        "id": "l-dXQhIP_OMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "Create a histogram of the group_size column of restaurant_groups, setting bins to [2, 3, 4, 5, 6]. Remember to show the plot.\n",
        "\n",
        "# Create a histogram of restaurant_groups and show plot\n",
        "restaurant_groups['group_size'].hist(bins=np.linspace(2,6,5))\n",
        "plt.show()\n",
        "\n",
        "Count the number of each group_size in restaurant_groups, then divide by the number of rows in restaurant_groups to calculate the probability of randomly selecting a group of each size. Save as size_dist.\n",
        "Reset the index of size_dist.\n",
        "Rename the columns of size_dist to group_size and prob.\n",
        "\n",
        "\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "print(size_dist)\n",
        "\n",
        "\n",
        "Calculate the expected value of the size_distribution, which represents the expected group size, by multiplying the group_size by the prob and taking the sum.\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "# Expected value\n",
        "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
        "print(expected_value)\n",
        "\n",
        "\n",
        "Calculate the probability of randomly picking a group of 4 or more people by subsetting for groups of size 4 or more and summing the probabilities of selecting those groups.\n",
        "\n",
        "# Create probability distribution\n",
        "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
        "# Reset index and rename columns\n",
        "size_dist = size_dist.reset_index()\n",
        "size_dist.columns = ['group_size', 'prob']\n",
        "\n",
        "# Expected value\n",
        "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
        "\n",
        "# Subset groups of size 4 or more\n",
        "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
        "\n",
        "# Sum the probabilities of groups_4_or_more\n",
        "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
        "print(prob_4_or_more)"
      ],
      "metadata": {
        "id": "_JBy919tEx5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "To model how long Amir will wait for a back-up using a continuous uniform distribution, save his lowest possible wait time as min_time and his longest possible wait time as max_time. Remember that back-ups happen every 30 minutes.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "\n",
        "Import uniform from scipy.stats and calculate the probability that Amir has to wait less than 5 minutes, and store in a variable called prob_less_than_5.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting less than 5 mins\n",
        "prob_less_than_5 = uniform.cdf(5, min_time, max_time)\n",
        "print(prob_less_than_5)\n",
        "\n",
        "Calculate the probability that Amir has to wait more than 5 minutes, and store in a variable called prob_greater_than_5.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting more than 5 mins\n",
        "prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\n",
        "print(prob_greater_than_5)\n",
        "\n",
        "\n",
        "Calculate the probability that Amir has to wait between 10 and 20 minutes, and store in a variable called prob_between_10_and_20.\n",
        "\n",
        "# Min and max wait times for back-up that happens every 30 min\n",
        "min_time = 0\n",
        "max_time = 30\n",
        "\n",
        "# Import uniform from scipy.stats\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Calculate probability of waiting 10-20 mins\n",
        "prob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\n",
        "print(prob_between_10_and_20)\n"
      ],
      "metadata": {
        "id": "E4tGTSWe8OEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Set the random seed to 334\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Import uniform from scipy.stats.\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Generate 1000 wait times from the continuous uniform distribution that models Amir's wait time. Save this as wait_times.\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Generate 1000 wait times between 0 and 30 mins\n",
        "wait_times = uniform.rvs(0, 30, size=1000)\n",
        "\n",
        "print(wait_times)\n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Create a histogram of the simulated wait times and show the plot.\n",
        "\n",
        "# Set random seed to 334\n",
        "np.random.seed(334)\n",
        "\n",
        "# Import uniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Generate 1000 wait times between 0 and 30 mins\n",
        "wait_times = uniform.rvs(0, 30, size=1000)\n",
        "\n",
        "# Create a histogram of simulated times and show plot\n",
        "plt.hist(wait_times)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G0Yvewva_nGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nstructions 1/4\n",
        "25 XP\n",
        "1\n",
        "Import binom from scipy.stats and set the random seed to 10.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "\n",
        "Simulate 1 deal worked on by Amir, who wins 30% of the deals he works on.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate a single deal\n",
        "print(binom.rvs(1, 0.3, size=1))\n",
        "\n",
        "Simulate a typical week of Amir's deals, or one week of 3 deals.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate 1 week of 3 deals\n",
        "print(binom.rvs(3, 0.3, size=1))\n",
        "\n",
        "\n",
        "Simulate a year's worth of Amir's deals, or 52 weeks of 3 deals each, and store in deals.\n",
        "Print the mean number of deals he won per week.\n",
        "\n",
        "# Import binom from scipy.stats\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Set random seed to 10\n",
        "np.random.seed(10)\n",
        "\n",
        "# Simulate 52 weeks of 3 deals\n",
        "deals = binom.rvs(3, 0.3, size=52)\n",
        "\n",
        "# Print mean deals won per week\n",
        "print(np.mean(deals))\n"
      ],
      "metadata": {
        "id": "B-lblgbS_nCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "What's the probability that Amir closes all 3 deals in a week? Save this as prob_3.\n",
        "\n",
        "# Probability of closing 3 out of 3 deals\n",
        "prob_3 = binom.pmf(3, 3, 0.3)\n",
        "\n",
        "print(prob_3)\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "What's the probability that Amir closes 1 or fewer deals in a week? Save this as prob_less_than_or_equal_1.\n",
        "\n",
        "# Probability of closing <= 1 deal out of 3 deals\n",
        "prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n",
        "\n",
        "print(prob_less_than_or_equal_1)\n",
        "\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "What's the probability that Amir closes more than 1 deal? Save this as prob_greater_than_1.\n",
        "\n",
        "# Probability of closing > 1 deal out of 3 deals\n",
        "prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)\n",
        "\n",
        "print(prob_greater_than_1)\n"
      ],
      "metadata": {
        "id": "APaxcaxi8OBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Calculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.\n",
        "Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate drops to 25%.\n",
        "Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate rises to 35%.\n",
        "\n",
        "# Expected number won with 30% win rate\n",
        "won_30pct = 3 * 0.3\n",
        "print(won_30pct)\n",
        "\n",
        "# Expected number won with 25% win rate\n",
        "won_25pct = 3 * 0.25\n",
        "print(won_25pct)\n",
        "\n",
        "# Expected number won with 35% win rate\n",
        "won_35pct = 3 * 0.35\n",
        "print(won_35pct)"
      ],
      "metadata": {
        "id": "zVLXlpIz8N-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "vcSGp_9kqvWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Create a histogram with 10 bins to visualize the distribution of the amount. Show the plot.\n",
        "\n",
        "\n",
        "# Histogram of amount with 10 bins and show plot\n",
        "amir_deals['amount'].hist(bins=10)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QGY0A2rV8N4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "1 XP\n",
        "1\n",
        "What's the probability of Amir closing a deal worth less than $7500?\n",
        "\n",
        "# Probability of deal < 7500\n",
        "prob_less_7500 = norm.cdf(7500, 5000, 2000)\n",
        "\n",
        "print(prob_less_7500)\n",
        "\n",
        "\n",
        "\n",
        "What's the probability of Amir closing a deal worth more than $1000?\n",
        "\n",
        "\n",
        "# Probability of deal > 1000\n",
        "prob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)\n",
        "\n",
        "print(prob_over_1000)\n",
        "\n",
        "\n",
        "What's the probability of Amir closing a deal worth between $3000 and $7000?\n",
        "\n",
        "# Probability of deal between 3000 and 7000\n",
        "prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\n",
        "\n",
        "print(prob_3000_to_7000)\n",
        "\n",
        "\n",
        "What amount will 25% of Amir's sales be less than?\n",
        "\n",
        "# Calculate amount that 25% of deals will be less than\n",
        "pct_25 = norm.ppf(0.25, 5000, 2000)\n",
        "\n",
        "print(pct_25)"
      ],
      "metadata": {
        "id": "64m39ZmpwJkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Currently, Amir's average sale amount is $5000. Calculate what his new average amount will be if it increases by 20% and store this in new_mean.\n",
        "Amir's current standard deviation is $2000. Calculate what his new standard deviation will be if it increases by 30% and store this in new_sd.\n",
        "Create a variable called new_sales, which contains 36 simulated amounts from a normal distribution with a mean of new_mean and a standard deviation of new_sd.\n",
        "Plot the distribution of the new_sales amounts using a histogram and show the plot.\n",
        "\n",
        "\n",
        "# Calculate new average amount\n",
        "new_mean = 5000 * 1.2\n",
        "\n",
        "# Calculate new standard deviation\n",
        "new_sd = 2000 * 1.3\n",
        "\n",
        "# Simulate 36 new sales\n",
        "new_sales = norm.rvs(new_mean, new_sd, size=36)\n",
        "\n",
        "# Create histogram and show\n",
        "plt.hist(new_sales)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "imDueSoQwJgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Set the random seed to 321.\n",
        "Take 30 samples (with replacement) of size 20 from all_deals['num_users'] and take the mean of each sample. Store the sample means in sample_means.\n",
        "Print the mean of sample_means.\n",
        "Print the mean of the num_users column of amir_deals.\n",
        "\n",
        "# Set seed to 321\n",
        "np.random.seed(321)\n",
        "\n",
        "sample_means = []\n",
        "# Loop 30 times to take 30 means\n",
        "for i in range(30):\n",
        "  # Take sample of size 20 from num_users col of all_deals with replacement\n",
        "  cur_sample = all_deals['num_users'].sample(20, replace=True)\n",
        "  # Take mean of cur_sample\n",
        "  cur_mean = np.mean(cur_sample)\n",
        "  # Append cur_mean to sample_means\n",
        "  sample_means.append(cur_mean)\n",
        "\n",
        "# Print mean of sample_means\n",
        "print(np.mean(sample_means))\n",
        "\n",
        "# Print mean of num_users in amir_deals\n",
        "print(np.mean(amir_deals['num_users']))\n"
      ],
      "metadata": {
        "id": "9IjuoItzzrpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fFwAfEHUzrWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Import poisson from scipy.stats and calculate the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4.\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of 5 responses\n",
        "prob_5 = poisson.pmf(5, 4)\n",
        "\n",
        "print(prob_5)\n",
        "\n",
        "\n",
        "Amir's coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of 5 responses\n",
        "prob_coworker = poisson.pmf(5, 5.5)\n",
        "\n",
        "print(prob_coworker)\n",
        "\n",
        "\n",
        "What's the probability that Amir responds to 2 or fewer leads in a day?\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of 2 or fewer responses\n",
        "prob_2_or_less = poisson.cdf(2, 4)\n",
        "\n",
        "print(prob_2_or_less)\n",
        "\n",
        "What's the probability that Amir responds to more than 10 leads in a day?\n",
        "\n",
        "# Import poisson from scipy.stats\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Probability of > 10 responses\n",
        "prob_over_10 = 1 - poisson.cdf(10, 4)\n",
        "\n",
        "print(prob_over_10)"
      ],
      "metadata": {
        "id": "B4ClElcD6kGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "1 XP\n",
        "Import expon from scipy.stats. What's the probability it takes Amir less than an hour to respond to a lead?\n",
        "\n",
        "\n",
        "# Import expon from scipy.stats\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Print probability response takes < 1 hour\n",
        "print(expon.cdf(1, scale=2.5))\n",
        "\n",
        "\n",
        "What's the probability it takes Amir more than 4 hours to respond to a lead?\n",
        "\n",
        "# Import expon from scipy.stats\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Print probability response takes > 4 hours\n",
        "print(1 - expon.cdf(4, scale=2.5))\n",
        "\n",
        "\n",
        "What's the probability it takes Amir 3-4 hours to respond to a lead?\n",
        "\n",
        "# Import expon from scipy.stats\n",
        "from scipy.stats import expon\n",
        "\n",
        "# Print probability response takes 3-4 hours\n",
        "print(expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5))\n"
      ],
      "metadata": {
        "id": "47BdGCxF6kC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "bp1uKqQ1EjVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Create a scatterplot of happiness_score vs. life_exp (without a trendline) using seaborn.\n",
        "Show the plot.\n",
        "\n",
        "# Create a scatterplot of happiness_score vs. life_exp and show\n",
        "sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Create a scatterplot of happiness_score vs. life_exp with a linear trendline using seaborn, setting ci to None.\n",
        "Show the plot.\n",
        "\n",
        "# Create scatterplot of happiness_score vs life_exp with trendline\n",
        "sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "g_QbEzEvEjzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Create a seaborn scatterplot (without a trendline) showing the relationship between gdp_per_cap (on the x-axis) and life_exp (on the y-axis).\n",
        "Show the plot\n",
        "\n",
        "# Scatterplot of gdp_per_cap and life_exp\n",
        "sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Calculate the correlation between gdp_per_cap and life_exp and store as cor.\n",
        "\n",
        "\n",
        "# Scatterplot of gdp_per_cap and life_exp\n",
        "sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "  \n",
        "# Correlation between gdp_per_cap and life_exp\n",
        "cor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n",
        "\n",
        "print(cor)\n",
        "\n"
      ],
      "metadata": {
        "id": "_e91gANbwJcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a scatterplot of happiness_score versus gdp_per_cap and calculate the correlation between them.\n",
        "\n",
        "# Scatterplot of happiness_score vs. gdp_per_cap\n",
        "sns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "cor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
        "print(cor)\n",
        "\n",
        "\n",
        "Add a new column to world_happiness called log_gdp_per_cap that contains the log of gdp_per_cap.\n",
        "Create a seaborn scatterplot of log_gdp_per_cap and happiness_score and calculate the correlation between them.\n",
        "\n",
        "# Create log_gdp_per_cap column\n",
        "world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n",
        "\n",
        "# Scatterplot of log_gdp_per_cap and happiness_score\n",
        "sns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
        "print(cor)\n"
      ],
      "metadata": {
        "id": "fg6yx6T3wJYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "1\n",
        "2\n",
        "Create a seaborn scatterplot showing the relationship between grams_sugar_per_day (on the x-axis) and happiness_score (on the y-axis).\n",
        "Calculate the correlation between grams_sugar_per_day and happiness_score.\n",
        "\n",
        "\n",
        "# Scatterplot of grams_sugar_per_day and happiness_score\n",
        "sns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\n",
        "plt.show()\n",
        "\n",
        "# Correlation between grams_sugar_per_day and happiness_score\n",
        "cor = world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\n",
        "print(cor)\n",
        "\n"
      ],
      "metadata": {
        "id": "vFZ0ugdib1Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONT**"
      ],
      "metadata": {
        "id": "TkKnLiM5B3K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 70 rows using simple random sampling and set the seed\n",
        "attrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_samp)"
      ],
      "metadata": {
        "id": "bO4Mhj1Db1O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Set the sample size to 70.\n",
        "Calculate the population size from attrition_pop.\n",
        "Calculate the interval between the rows to be sampled.\n",
        "\n",
        "# Set the sample size to 70\n",
        "sample_size = 70\n",
        "\n",
        "# Calculate the population size from attrition_pop\n",
        "pop_size = len(attrition_pop)\n",
        "\n",
        "# Calculate the interval\n",
        "interval = pop_size // sample_size\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Systematically sample attrition_pop to get the rows of the population at each interval, starting at 0; assign the rows to attrition_sys_samp.\n",
        "\n",
        "# Set the sample size to 70\n",
        "sample_size = 70\n",
        "\n",
        "# Calculate the population size from attrition_pop\n",
        "pop_size = len(attrition_pop)\n",
        "\n",
        "# Calculate the interval\n",
        "interval = pop_size // sample_size\n",
        "\n",
        "# Systematically sample 70 rows\n",
        "attrition_sys_samp = attrition_pop.iloc[::interval]\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_sys_samp)"
      ],
      "metadata": {
        "id": "LkvubHPtB2nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "1 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Add an index column to attrition_pop, assigning the result to attrition_pop_id.\n",
        "Create a scatter plot of YearsAtCompany versus index for attrition_pop_id using pandas .plot().\n",
        "\n",
        "# Add an index column to attrition_pop\n",
        "attrition_pop_id = attrition_pop.reset_index()\n",
        "\n",
        "# Plot YearsAtCompany vs. index for attrition_pop_id\n",
        "attrition_pop_id.plot(x=\"index\", y=\"YearsAtCompany\", kind=\"scatter\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Randomly shuffle the rows of attrition_pop.\n",
        "Reset the row indexes, and add an index column to attrition_pop.\n",
        "Repeat the scatter plot of YearsAtCompany versus index, this time using attrition_shuffled.\n",
        "\n",
        "\n",
        "# Shuffle the rows of attrition_pop\n",
        "attrition_shuffled = attrition_pop.sample(frac=1)\n",
        "\n",
        "# Reset the row indexes and create an index column\n",
        "attrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n",
        "\n",
        "# Plot YearsAtCompany vs. index for attrition_shuffled\n",
        "attrition_shuffled.plot(x=\"index\", y=\"YearsAtCompany\", kind=\"scatter\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rX-s3qN-B2j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Get the proportion of employees by Education level from attrition_pop.\n",
        "\n",
        "\n",
        "# Proportion of employees by Education level\n",
        "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_pop\n",
        "print(education_counts_pop)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Use proportional stratified sampling on attrition_pop to sample 40% of each Education group, setting the seed to 2022.\n",
        "\n",
        "# Proportion of employees by Education level\n",
        "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_pop\n",
        "print(education_counts_pop)\n",
        "\n",
        "# Proportional stratified sampling for 40% of each Education group\n",
        "attrition_strat = attrition_pop.groupby('Education')\\\n",
        "\t.sample(frac=0.4, random_state=2022)\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_strat)\n",
        "\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Get the proportion of employees by Education level from attrition_strat.\n",
        "\n",
        "# Proportion of employees by Education level\n",
        "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_pop\n",
        "print(education_counts_pop)\n",
        "\n",
        "# Proportional stratified sampling for 40% of each Education group\n",
        "attrition_strat = attrition_pop.groupby('Education')\\\n",
        "\t.sample(frac=0.4, random_state=2022)\n",
        "\n",
        "# Calculate the Education level proportions from attrition_strat\n",
        "education_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n",
        "\n",
        "# Print education_counts_strat\n",
        "print(education_counts_strat)"
      ],
      "metadata": {
        "id": "YJzHnGJNB2gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Use equal counts stratified sampling on attrition_pop to get 30 employees from each Education group, setting the seed to 2022.\n",
        "\n",
        "\n",
        "# Get 30 employees from each Education group\n",
        "attrition_eq = attrition_pop.groupby('Education')\\\n",
        "\t.sample(n=30, random_state=2022)      \n",
        "         \n",
        "# Print the sample\n",
        "print(attrition_eq)\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Get the proportion of employees by Education level from attrition_eq.\n",
        "\n",
        "# Get 30 employees from each Education group\n",
        "attrition_eq = attrition_pop.groupby('Education')\\\n",
        "\t.sample(n=30, random_state=2022)      \n",
        "\n",
        "# Get the proportions from attrition_eq\n",
        "education_counts_eq = attrition_eq['Education'].value_counts(normalize=True) \n",
        "\n",
        "# Print the results\n",
        "print(education_counts_eq)\n"
      ],
      "metadata": {
        "id": "1HvCQ_LMB2O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "\n",
        "Plot YearsAtCompany from attrition_pop as a histogram with bins of width 1 from 0 to 40.\n",
        "\n",
        "# Plot YearsAtCompany from attrition_pop as a histogram\n",
        "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
        "plt.show()\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Sample 400 employees from attrition_pop weighted by YearsAtCompany.\n",
        "\n",
        "# Plot YearsAtCompany from attrition_pop as a histogram\n",
        "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
        "plt.show()\n",
        "\n",
        "# Sample 400 employees weighted by YearsAtCompany\n",
        "attrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_weight)\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Plot YearsAtCompany from attrition_weight as a histogram with bins of width 1 from 0 to 40.\n",
        "\n",
        "# Plot YearsAtCompany from attrition_pop as a histogram\n",
        "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
        "plt.show()\n",
        "\n",
        "# Sample 400 employees weighted by YearsAtCompany\n",
        "attrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n",
        "\n",
        "# Plot YearsAtCompany from attrition_weight as a histogram\n",
        "attrition_weight['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJeOFjtYB2L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Create a list of unique JobRole values from attrition_pop, and assign to job_roles_pop.\n",
        "Randomly sample four JobRole values from job_roles_pop.\n",
        "\n",
        "# Create a list of unique JobRole values\n",
        "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
        "\n",
        "# Randomly sample four JobRole values\n",
        "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
        "\n",
        "# Print the result\n",
        "print(job_roles_samp)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Subset attrition_pop for the sampled job roles by filtering for rows where JobRole is in job_roles_samp.\n",
        "\n",
        "# Create a list of unique JobRole values\n",
        "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
        "\n",
        "# Randomly sample four JobRole values\n",
        "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
        "\n",
        "# Filter for rows where JobRole is in job_roles_samp\n",
        "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
        "attrition_filtered = attrition_pop[jobrole_condition]\n",
        "\n",
        "# Print the result\n",
        "print(attrition_filtered)\n",
        "\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Remove any unused categories from JobRole.\n",
        "For each job role in the filtered dataset, take a random sample of ten rows, setting the seed to 2022.\n",
        "\n",
        "# Create a list of unique JobRole values\n",
        "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
        "\n",
        "# Randomly sample four JobRole values\n",
        "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
        "\n",
        "# Filter for rows where JobRole is in job_roles_samp\n",
        "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
        "attrition_filtered = attrition_pop[jobrole_condition]\n",
        "\n",
        "# Remove categories with no rows\n",
        "attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n",
        "\n",
        "# Randomly sample 10 employees from each sampled job role\n",
        "attrition_clust = attrition_filtered.groupby(\"JobRole\")\\\n",
        "    .sample(n=10, random_state=2022)\n",
        "\n",
        "# Print the sample\n",
        "print(attrition_clust)"
      ],
      "metadata": {
        "id": "Zq9o2uMGxn2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "Perform simple random sampling on attrition_pop to get one-quarter of the population, setting the seed to 2022.\n",
        "\n",
        "# Perform simple random sampling to get 0.25 of the population\n",
        "attrition_srs = attrition_pop.sample(frac = 0.25, random_state=2022)\n",
        "\n",
        "\n",
        "Perform stratified sampling on attrition_pop to sample one-quarter of each RelationshipSatisfaction group, setting the seed to 2022.\n",
        "\n",
        "# Perform stratified sampling to get 0.25 of each relationship group\n",
        "attrition_strat = attrition_pop.groupby(\"RelationshipSatisfaction\")\\\n",
        "    .sample(frac=0.25, random_state=2022)\n",
        "\n",
        "\n",
        "\n",
        "Create a list of unique values from attrition_pop's RelationshipSatisfaction column.\n",
        "Randomly sample satisfaction_unique to get two values.\n",
        "Subset the population for rows where RelationshipSatisfaction is in satisfaction_samp and clear any unused categories from RelationshipSatisfaction; assign to attrition_clust_prep.\n",
        "Perform cluster sampling on the selected satisfaction groups, sampling one quarter of the population and setting the seed to 2022.\n",
        "\n",
        "# Create a list of unique RelationshipSatisfaction values\n",
        "satisfaction_unique = list(attrition_pop['RelationshipSatisfaction'].unique())\n",
        "\n",
        "# Randomly sample 2 unique satisfaction values\n",
        "satisfaction_samp = random.sample(satisfaction_unique, k=2)\n",
        "\n",
        "# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\n",
        "satis_condition = attrition_pop['RelationshipSatisfaction'].isin(satisfaction_samp)\n",
        "attrition_clust_prep = attrition_pop[satis_condition]\n",
        "attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n",
        "\n",
        "# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\n",
        "attrition_clust = attrition_clust_prep.groupby(\"RelationshipSatisfaction\")\\\n",
        "    .sample(n=len(attrition_pop) // 4, random_state=2022)"
      ],
      "metadata": {
        "id": "pjOESFHcxnxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "Group attrition_pop by RelationshipSatisfaction levels and calculate the mean of Attrition for each level.\n",
        "\n",
        "# Mean Attrition by RelationshipSatisfaction group\n",
        "mean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
        "\n",
        "# Print the result\n",
        "print(mean_attrition_pop)\n",
        "\n",
        "\n",
        "Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the simple random sample, attrition_srs.\n",
        "\n",
        "# Calculate the same thing for the simple random sample \n",
        "mean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
        "\n",
        "# Print the result\n",
        "print(mean_attrition_srs)\n",
        "\n",
        "\n",
        "Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the stratified sample, attrition_strat.\n",
        "\n",
        "# Calculate the same thing for the stratified sample \n",
        "mean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
        "\n",
        "# Print the result\n",
        "print(mean_attrition_strat)\n",
        "\n",
        "\n",
        "Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the cluster sample, attrition_clust.\n",
        "\n",
        "# Calculate the same thing for the cluster sample \n",
        "mean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')['Attrition'].mean()\n",
        "\n",
        "# Print the result\n",
        "print(mean_attrition_clust)\n",
        "\n"
      ],
      "metadata": {
        "id": "oQ__2nEwMLtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "Cr0C8JQhelK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Generate a simple random sample from attrition_pop of fifty rows, setting the seed to 2022.\n",
        "Calculate the mean employee Attrition in the sample.\n",
        "Calculate the relative error between mean_attrition_srs50 and mean_attrition_pop as a percentage.\n",
        "\n",
        "\n",
        "# Generate a simple random sample of 50 rows, with seed 2022\n",
        "attrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n",
        "\n",
        "# Calculate the mean employee attrition in the sample\n",
        "mean_attrition_srs50 = attrition_srs50['Attrition'].mean()\n",
        "\n",
        "# Calculate the relative error percentage\n",
        "rel_error_pct50 = 100 * abs(mean_attrition_pop - mean_attrition_srs50) / mean_attrition_pop\n",
        "\n",
        "# Print rel_error_pct50\n",
        "print(rel_error_pct50)\n",
        "\n",
        "\n",
        "Calculate the relative error percentage again. This time, use a simple random sample of one hundred rows of attrition_pop.\n",
        "\n",
        "\n",
        "# Generate a simple random sample of 100 rows, with seed 2022\n",
        "attrition_srs100 = attrition_pop.sample(n=100, random_state=2022)\n",
        "\n",
        "# Calculate the mean employee attrition in the sample\n",
        "mean_attrition_srs100 = attrition_srs100['Attrition'].mean()\n",
        "\n",
        "# Calculate the relative error percentage\n",
        "rel_error_pct100 = 100 * abs(mean_attrition_pop - mean_attrition_srs100) / mean_attrition_pop\n",
        "\n",
        "# Print rel_error_pct100\n",
        "print(rel_error_pct100)"
      ],
      "metadata": {
        "id": "XNx_BIkHMLpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Replicate the provided code so that it runs 500 times. Assign the resulting list of sample means to mean_attritions.\n",
        "\n",
        "# Create an empty list\n",
        "mean_attritions = []\n",
        "# Loop 500 times to create 500 sample means\n",
        "for i in range(500):\n",
        "\tmean_attritions.append(\n",
        "    \tattrition_pop.sample(n=60)['Attrition'].mean()\n",
        "\t)\n",
        "  \n",
        "# Print out the first few entries of the list\n",
        "print(mean_attritions[0:5])\n",
        "\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Draw a histogram of the mean_attritions list with 16 bins.\n",
        "\n",
        "\n",
        "# Create an empty list\n",
        "mean_attritions = []\n",
        "# Loop 500 times to create 500 sample means\n",
        "for i in range(500):\n",
        "\tmean_attritions.append(\n",
        "    \tattrition_pop.sample(n=60)['Attrition'].mean()\n",
        "\t)\n",
        "\n",
        "# Create a histogram of the 500 sample means\n",
        "plt.hist(mean_attritions, bins=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rVFt5JNue7B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Expand a grid representing 5 8-sided dice. That is, create a DataFrame with five columns from a dictionary, named die1 to die5. The rows should contain all possibilities for throwing five dice, each numbered 1 to 8.\n",
        "\n",
        "\n",
        "# Expand a grid representing 5 8-sided dice\n",
        "dice = expand_grid(\n",
        "  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "  })\n",
        "\n",
        "# Print the result\n",
        "print(dice)\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Add a column, mean_roll, to dice, that contains the mean of the five rolls as a categorical.\n",
        "\n",
        "\n",
        "# Expand a grid representing 5 8-sided dice\n",
        "dice = expand_grid(\n",
        "  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "  })\n",
        "\n",
        "# Add a column of mean rolls and convert to a categorical\n",
        "dice['mean_roll'] = (dice['die1'] + dice['die2'] + \n",
        "                     dice['die3'] + dice['die4'] + \n",
        "                     dice['die5']) / 5\n",
        "dice['mean_roll'] = dice['mean_roll'].astype('category')\n",
        "\n",
        "# Print result\n",
        "print(dice)\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Create a bar plot of the mean_roll categorical column, so it displays the count of each mean_roll in increasing order from 1.0 to 8.0.\n",
        "\n",
        "# Expand a grid representing 5 8-sided dice\n",
        "dice = expand_grid(\n",
        "  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "  })\n",
        "\n",
        "# Add a column of mean rolls and convert to a categorical\n",
        "dice['mean_roll'] = (dice['die1'] + dice['die2'] + \n",
        "                     dice['die3'] + dice['die4'] + \n",
        "                     dice['die5']) / 5\n",
        "dice['mean_roll'] = dice['mean_roll'].astype('category')\n",
        "\n",
        "# Draw a bar plot of mean_roll\n",
        "dice['mean_roll'].value_counts(sort=False).plot(kind=\"bar\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pMDa5B0yxnsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Sample one to eight, five times, with replacement. Assign to five_rolls.\n",
        "Calculate the mean of five_rolls.\n",
        "\n",
        "# Sample one to eight, five times, with replacement\n",
        "five_rolls = np.random.choice(list(range(1, 9)), size=5, replace=True)\n",
        "\n",
        "# Print the mean of five_rolls\n",
        "print(five_rolls.mean())\n",
        "\n",
        "\n",
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Replicate the sampling code 1000 times, assigning each result to the list sample_means_1000.\n",
        "\n",
        "# Replicate the sampling code 1000 times\n",
        "sample_means_1000 = []\n",
        "for i in range(1000):\n",
        "    sample_means_1000.append(\n",
        "  \t\tnp.random.choice(list(range(1, 9)), size=5, replace=True).mean()\n",
        "    )\n",
        "\n",
        "# Print the first 10 entries of the result\n",
        "print(sample_means_1000[0:10])\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Plot sample_means_1000 as a histogram with 20 bins.\n",
        "\n",
        "# Replicate the sampling code 1000 times\n",
        "sample_means_1000 = []\n",
        "for i in range(1000):\n",
        "    sample_means_1000.append(\n",
        "  \t\tnp.random.choice(list(range(1, 9)), size=5, replace=True).mean()\n",
        "    )\n",
        "\n",
        "# Draw a histogram of sample_means_1000 with 20 bins\n",
        "plt.hist(sample_means_1000, bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RM6X3Rnf2iQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Calculate the mean of sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 (a mean of sample means).\n",
        "\n",
        "\n",
        "# Calculate the mean of the mean attritions for each sampling distribution\n",
        "mean_of_means_5 = np.mean(sampling_distribution_5)\n",
        "mean_of_means_50 = np.mean(sampling_distribution_50)\n",
        "mean_of_means_500 = np.mean(sampling_distribution_500)\n",
        "\n",
        "# Print the results\n",
        "print(mean_of_means_5)\n",
        "print(mean_of_means_50)\n",
        "print(mean_of_means_500)\n"
      ],
      "metadata": {
        "id": "FfvqYzIiB1-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "Calculate the standard deviation of sampling_distribution_5, sampling_distribution_50, and sampling_distribution_500 (a standard deviation of sample means).\n",
        "\n",
        "\n",
        "# Calculate the std. dev. of the mean attritions for each sampling distribution\n",
        "sd_of_means_5 = np.std(sampling_distribution_5, ddof=1)\n",
        "sd_of_means_50 = np.std(sampling_distribution_50,ddof=1)\n",
        "sd_of_means_500 = np.std(sampling_distribution_500, ddof=1)\n",
        "\n",
        "# Print the results\n",
        "print(sd_of_means_5)\n",
        "print(sd_of_means_50)\n",
        "print(sd_of_means_500)"
      ],
      "metadata": {
        "id": "wKlc_JZkB16X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "94PDSnXFhjse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Generate a single bootstrap resample from spotify_sample.\n",
        "\n",
        "# Generate 1 bootstrap resample\n",
        "spotify_1_resample = spotify_sample.sample(frac=1, replace=True)\n",
        "\n",
        "# Print the resample\n",
        "print(spotify_1_resample)\n",
        "\n",
        "Instructions 2/4\n",
        "25 XP\n",
        "2\n",
        "\n",
        "Calculate the mean of the danceability column of spotify_1_resample using numpy.\n",
        "\n",
        "# Generate 1 bootstrap resample\n",
        "spotify_1_resample = spotify_sample.sample(frac=1, replace=True)\n",
        "\n",
        "# Calculate of the danceability column of spotify_1_resample\n",
        "mean_danceability_1 = np.mean(spotify_1_resample['danceability'])\n",
        "\n",
        "# Print the result\n",
        "print(mean_danceability_1)\n",
        "\n",
        "\n",
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Replicate the expression provided 1000 times.\n",
        "\n",
        "# Replicate this 1000 times\n",
        "mean_danceability_1000 = []\n",
        "for i in range(1000):\n",
        "\tmean_danceability_1000.append(\n",
        "        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])\n",
        "\t)\n",
        "  \n",
        "# Print the result\n",
        "print(mean_danceability_1000)\n",
        "\n",
        "\n",
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Create a bootstrap distribution by drawing a histogram of mean_danceability_1000.\n",
        "\n",
        "# Replicate this 1000 times\n",
        "mean_danceability_1000 = []\n",
        "for i in range(1000):\n",
        "\tmean_danceability_1000.append(\n",
        "        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])\n",
        "\t)\n",
        "\n",
        "# Draw a histogram of the resample means\n",
        "plt.hist(mean_danceability_1000)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F4w3AgHocGie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Generate a sampling distribution of 2000 replicates.\n",
        "Sample 500 rows of the population without replacement and calculate the mean popularity.\n",
        "\n",
        "\n",
        "mean_popularity_2000_samp = []\n",
        "\n",
        "# Generate a sampling distribution of 2000 replicates\n",
        "for i in range(2000):\n",
        "    mean_popularity_2000_samp.append(\n",
        "    \t# Sample 500 rows and calculate the mean popularity     \n",
        "    \tspotify_population.sample(n=500)['popularity'].mean()\n",
        "    )\n",
        "\n",
        "# Print the sampling distribution results\n",
        "print(mean_popularity_2000_samp)\n",
        "\n",
        "\n",
        "Generate a bootstrap distribution of 2000 replicates.\n",
        "Sample 500 rows of the sample with replacement and calculate the mean popularity.\n",
        "\n",
        "mean_popularity_2000_boot = []\n",
        "\n",
        "# Generate a bootstrap distribution of 2000 replicates\n",
        "for i in range(2000):\n",
        "    mean_popularity_2000_boot.append(\n",
        "    \t# Resample 500 rows and calculate the mean popularity\n",
        "    \tspotify_sample.sample(n=500, replace=True)['popularity'].mean()\n",
        "    )\n",
        "\n",
        "# Print the bootstrap distribution results\n",
        "print(mean_popularity_2000_boot)\n"
      ],
      "metadata": {
        "id": "VMm4KDSicGeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Calculate the mean popularity in 4 ways:\n",
        "\n",
        "Population: from spotify_population, take the mean of popularity.\n",
        "Sample: from spotify_sample, take the mean of popularity.\n",
        "Sampling distribution: from sampling_distribution, take its mean.\n",
        "Bootstrap distribution: from bootstrap_distribution, take its mean.\n",
        "\n",
        "# Calculate the population mean popularity\n",
        "pop_mean = spotify_population['popularity'].mean()\n",
        "\n",
        "# Calculate the original sample mean popularity\n",
        "samp_mean = spotify_sample['popularity'].mean()\n",
        "\n",
        "# Calculate the sampling dist'n estimate of mean popularity\n",
        "samp_distn_mean = np.mean(sampling_distribution)\n",
        "\n",
        "# Calculate the bootstrap dist'n estimate of mean popularity\n",
        "boot_distn_mean = np.mean(bootstrap_distribution)\n",
        "\n",
        "# Print the means\n",
        "print([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])\n",
        "\n"
      ],
      "metadata": {
        "id": "lCdew09QcGZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "1\n",
        "2\n",
        "Calculate the mean popularity in 4 ways:\n",
        "\n",
        "Population: from spotify_population, take the mean of popularity.\n",
        "Sample: from spotify_sample, take the mean of popularity.\n",
        "Sampling distribution: from sampling_distribution, take its mean.\n",
        "Bootstrap distribution: from bootstrap_distribution, take its mean.\n",
        "\n",
        "\n",
        "# Calculate the population mean popularity\n",
        "pop_mean = spotify_population['popularity'].mean()\n",
        "\n",
        "# Calculate the original sample mean popularity\n",
        "samp_mean = spotify_sample['popularity'].mean()\n",
        "\n",
        "# Calculate the sampling dist'n estimate of mean popularity\n",
        "samp_distn_mean = np.mean(sampling_distribution)\n",
        "\n",
        "# Calculate the bootstrap dist'n estimate of mean popularity\n",
        "boot_distn_mean = np.mean(bootstrap_distribution)\n",
        "\n",
        "# Print the means\n",
        "print([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e9N2xLc6DCAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Calculate the standard deviation of popularity in 4 ways.\n",
        "\n",
        "Population: from spotify_population, take the standard deviation of popularity.\n",
        "Original sample: from spotify_sample, take the standard deviation of popularity.\n",
        "Sampling distribution: from sampling_distribution, take its standard deviation and multiply by the square root of the sample size (5000).\n",
        "Bootstrap distribution: from bootstrap_distribution, take its standard deviation and multiply by the square root of the sample size.\n",
        "\n",
        "# Calculate the population std dev popularity\n",
        "pop_sd = spotify_population['popularity'].std(ddof=0)\n",
        "\n",
        "# Calculate the original sample std dev popularity\n",
        "samp_sd = spotify_sample['popularity'].std()\n",
        "\n",
        "# Calculate the sampling dist'n estimate of std dev popularity\n",
        "samp_distn_sd = np.std(sampling_distribution, ddof=1) * np.sqrt(5000)\n",
        "\n",
        "# Calculate the bootstrap dist'n estimate of std dev popularity\n",
        "boot_distn_sd = np.std(bootstrap_distribution, ddof=1) * np.sqrt(5000)\n",
        "\n",
        "# Print the standard deviations\n",
        "print([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a-_zVw5BDAqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Generate a 95% confidence interval using the quantile method on the bootstrap distribution, setting the 0.025 quantile as lower_quant and the 0.975 quantile as upper_quant.\n",
        "\n",
        "\n",
        "# Generate a 95% confidence interval using the quantile method\n",
        "lower_quant = np.quantile(bootstrap_distribution, 0.025)\n",
        "upper_quant = np.quantile(bootstrap_distribution, 0.975)\n",
        "\n",
        "# Print quantile method confidence interval\n",
        "print((lower_quant, upper_quant))\n",
        "\n",
        "\n",
        "Generate a 95% confidence interval using the standard error method from the bootstrap distribution.\n",
        "\n",
        "Calculate point_estimate as the mean of bootstrap_distribution, and standard_error as the standard deviation of bootstrap_distribution.\n",
        "Calculate lower_se as the 0.025 quantile of an inv. CDF from a normal distribution with mean point_estimate and standard deviation standard_error.\n",
        "Calculate upper_se as the 0.975 quantile of that same inv. CDF.\n",
        "\n",
        "\n",
        "# Find the mean and std dev of the bootstrap distribution\n",
        "point_estimate = np.mean(bootstrap_distribution)\n",
        "standard_error = np.std(bootstrap_distribution, ddof=1)\n",
        "\n",
        "# Find the lower limit of the confidence interval\n",
        "lower_se = norm.ppf(0.025, loc=point_estimate, scale=standard_error)\n",
        "\n",
        "# Find the upper limit of the confidence interval\n",
        "upper_se = norm.ppf(0.975, loc=point_estimate, scale=standard_error)\n",
        "\n",
        "# Print standard error method confidence interval\n",
        "print((lower_se, upper_se))"
      ],
      "metadata": {
        "id": "RIYVhXzYDAIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6G0afP7C-BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAl5TP6GC7Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDSq1EaEC57_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XX3qaz3bC5jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FW-0wvnmY22T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9N14psaY2uY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}