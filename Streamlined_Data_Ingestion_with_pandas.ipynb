{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBpd1xB9TqDclgJds63I9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia07/DataCamp/blob/main/Streamlined_Data_Ingestion_with_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypBHGgl_tJmc"
      },
      "outputs": [],
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import the pandas library as pd.\n",
        "Use read_csv() to load vt_tax_data_2016.csv and assign it to the variable data.\n",
        "View the first few lines of the dataframe with the head() method. This code has been written for you.\n",
        "\n",
        "# Import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV and assign it to the variable data\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\")\n",
        "\n",
        "# View the first few lines of data\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Import pandas with the alias pd.\n",
        "Load vt_tax_data_2016.tsv, making sure to set the correct delimiter with the sep keyword argument.\n",
        "\n",
        "# Import pandas with the alias pd\n",
        "import pandas as pd\n",
        "\n",
        "# Load TSV using the sep keyword argument to set delimiter\n",
        "data = pd.read_csv(\"vt_tax_data_2016.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Plot the total number of tax returns by income group\n",
        "counts = data.groupby(\"agi_stub\").N1.sum()\n",
        "counts.plot.bar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wqP0BmmEtcRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a list of columns to use: zipcode, agi_stub (income group), mars1 (number of single households), MARS2 (number of households filing as married), and NUMDEP (number of dependents).\n",
        "Create a dataframe from vt_tax_data_2016.csv that uses only the selected columns.\n",
        "\n",
        "\n",
        "# Create list of columns to use\n",
        "cols = [\"zipcode\", \"agi_stub\", \"mars1\", \"MARS2\", \"NUMDEP\"]\n",
        "\n",
        "# Create dataframe from csv using only selected columns\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
        "\n",
        "# View counts of dependents and tax returns by income level\n",
        "print(data.groupby(\"agi_stub\").sum())\n"
      ],
      "metadata": {
        "id": "ZxJ7CIEatcN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Use nrows and skiprows to make a dataframe, vt_data_next500, with the next 500 rows.\n",
        "Set the header argument so that pandas knows there is no header row.\n",
        "Name the columns in vt_data_next500 by supplying a list of vt_data_first500's columns to the names argument.\n",
        "\n",
        "# Create dataframe of next 500 rows with labeled columns\n",
        "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\",\n",
        "                       \t\t  nrows=500,\n",
        "                       \t\t  skiprows=500,\n",
        "                       \t\t  header=None,\n",
        "                       \t\t  names=list(vt_data_first500))\n",
        "\n",
        "# View the Vermont dataframes to confirm they're different\n",
        "print(vt_data_first500.head())\n",
        "print(vt_data_next500.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "S_rDnggxteOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Load vt_tax_data_2016.csv with no arguments and view the dataframe's dtypes attribute. Note the data types of zipcode and agi_stub.\n",
        "\n",
        "# Load csv with no additional arguments\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\")\n",
        "\n",
        "# Print the data types\n",
        "print(data.dtypes)\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Create a dictionary, data_types, specifying that agi_stub is \"category\" data and zipcode is string data.\n",
        "Reload the CSV with the dtype argument and the dictionary to set the correct column data types.\n",
        "View the dataframe's dtypes attribute.\n",
        "\n",
        "# Create dict specifying data types for agi_stub and zipcode\n",
        "data_types = {\"agi_stub\": \"category\",\n",
        "\t\t\t  \"zipcode\": str}\n",
        "\n",
        "# Load csv using dtype to set correct data types\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
        "\n",
        "# Print data types of resulting frame\n",
        "print(data.dtypes.head())\n"
      ],
      "metadata": {
        "id": "eFttUidEtcL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a dictionary, null_values, specifying that 0s in the zipcode column should be considered NA values.\n",
        "Load vt_tax_data_2016.csv, using the na_values argument and the dictionary to make sure invalid ZIP codes are treated as missing.\n",
        "\n",
        "\n",
        "# Create dict specifying that 0s in zipcode are NA values\n",
        "null_values = {\"zipcode\": 0}\n",
        "\n",
        "# Load csv using na_values keyword argument\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\",\n",
        "                   na_values=null_values)\n",
        "\n",
        "# View rows with NA ZIP codes\n",
        "print(data[data.zipcode.isna()])\n",
        "\n"
      ],
      "metadata": {
        "id": "nTKaZpeFtcJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a dictionary, null_values, specifying that 0s in the zipcode column should be considered NA values.\n",
        "Load vt_tax_data_2016.csv, using the na_values argument and the dictionary to make sure invalid ZIP codes are treated as missing.\n",
        "\n",
        "# Create dict specifying that 0s in zipcode are NA values\n",
        "null_values = {\"zipcode\": 0}\n",
        "\n",
        "# Load csv using na_values keyword argument\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\",\n",
        "                   na_values=null_values)\n",
        "\n",
        "# View rows with NA ZIP codes\n",
        "print(data[data.zipcode.isna()])\n"
      ],
      "metadata": {
        "id": "7P_d38pAFB_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Try to import the file vt_tax_data_2016_corrupt.csv without any keyword arguments.\n",
        "\n",
        "\n",
        " # Import the CSV without any keyword arguments\n",
        "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\")\n",
        "\n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")\n",
        "\n",
        "\n",
        "Import vt_tax_data_2016_corrupt.csv with the error_bad_lines parameter set to skip bad records.\n",
        "\n",
        "\n",
        "try:\n",
        "  # Import CSV with error_bad_lines set to skip bad records\n",
        "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\",\n",
        "                     error_bad_lines=False)\n",
        "\n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")\n",
        "\n",
        "\n",
        "Update the import with the warn_bad_lines parameter set to issue a warning whenever a bad record is skipped.\n",
        "\n",
        "try:\n",
        "  # Set warn_bad_lines to issue warnings about bad records\n",
        "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\",\n",
        "                     error_bad_lines=False,\n",
        "                     warn_bad_lines=True)\n",
        "\n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")\n"
      ],
      "metadata": {
        "id": "OfsMYF5NFB7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Load the pandas library as pd.\n",
        "Read in fcc_survey.xlsx and assign it to the variable survey_responses.\n",
        "Print the first few records of survey_responses.\n",
        "\n",
        "# Load pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Read spreadsheet and assign it to survey_responses\n",
        "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
        "\n",
        "# View the head of the dataframe\n",
        "print(survey_responses.head())\n"
      ],
      "metadata": {
        "id": "ZqMULujwDgwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a single string, col_string, specifying that pandas should load column AD and the range AW through BA.\n",
        "Load fcc_survey_headers.xlsx', setting skiprows and usecols to skip the first two rows of metadata and get only the columns in col_string.\n",
        "View the selected column names in the resulting dataframe.\n",
        "\n",
        "# Create string of lettered columns to load\n",
        "col_string = \"AD, AW:BA\"\n",
        "\n",
        "# Load data with skiprows and usecols set\n",
        "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\",\n",
        "                                 skiprows=2,\n",
        "                                 usecols=col_string)\n",
        "\n",
        "# View the names of the columns selected\n",
        "print(survey_responses.columns)"
      ],
      "metadata": {
        "id": "AEZHimVkDgs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a dataframe from the second workbook sheet by passing the sheet's position to sheet_name.\n",
        "\n",
        "# Create df from second worksheet by referencing its position\n",
        "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                               sheet_name=1)\n",
        "\n",
        "# Graph where people would like to get a developer job\n",
        "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
        "job_prefs.plot.barh()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Create a dataframe from the 2017 sheet by providing the sheet's name to read_excel().\n",
        "\n",
        "# Create df from second worksheet by referencing its name\n",
        "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                               sheet_name=\"2017\")\n",
        "\n",
        "# Graph where people would like to get a developer job\n",
        "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
        "job_prefs.plot.barh()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y6SGERIgDgp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/3\n",
        "1 XP\n",
        "Load both the 2016 and 2017 sheets by name with a list and one call to read_excel().\n",
        "\n",
        "# Load both the 2016 and 2017 sheets by name\n",
        "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                                sheet_name=['2016', '2017'])\n",
        "\n",
        "# View the data type of all_survey_data\n",
        "print(type(all_survey_data))\n",
        "\n",
        "\n",
        "Load the 2016 sheet by its position (0) and 2017 by name. Note the sheet names in the result.\n",
        "\n",
        "# Load all sheets in the Excel file\n",
        "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                                sheet_name=[0, '2017'])\n",
        "\n",
        "# View the sheet names in all_survey_data\n",
        "print(all_survey_data.keys())\n",
        "\n",
        "\n",
        "´Load all sheets in the Excel file without listing them all.\n",
        "\n",
        "\n",
        "# Load all sheets in the Excel file\n",
        "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                                sheet_name=None)\n",
        "\n",
        "# View the sheet names in all_survey_data\n",
        "print(all_survey_data.keys())\n"
      ],
      "metadata": {
        "id": "k7ourESDIkIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create an empty dataframe, all_responses.\n",
        "Set up a for loop to iterate through the values in the responses dictionary.\n",
        "Append each dataframe to all_responses and reassign the result to the same variable name.\n",
        "\n",
        "\n",
        "# Create an empty dataframe\n",
        "all_responses = pd.DataFrame()\n",
        "\n",
        "# Set up for loop to iterate through values in responses\n",
        "for df in responses.values():\n",
        "  # Print the number of rows being added\n",
        "  print(\"Adding {} rows\".format(df.shape[0]))\n",
        "  # Append df to all_responses, assign result\n",
        "  all_responses = all_responses.append(df)\n",
        "\n",
        "# Graph employment statuses in sample\n",
        "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
        "counts.plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r2lRBHuJC3HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Count NA values in each column of survey_data with isna() and sum(). Note which columns besides ID.x, if any, have zero NAs.\n",
        "\n",
        "# Load the data\n",
        "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\")\n",
        "\n",
        "# Count NA values in each column\n",
        "print(survey_data.isna().sum())\n",
        "\n",
        "\n",
        "Set read_excel()'s dtype argument to load the HasDebt column as Boolean data.\n",
        "Supply the Boolean column name to the print statement to view financial burdens by group.\n",
        "\n",
        "# Set dtype to load appropriate column(s) as Boolean data\n",
        "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
        "                            dtype={\"HasDebt\": bool})\n",
        "\n",
        "# View financial burdens by Boolean group\n",
        "print(survey_data.groupby(\"HasDebt\").sum())"
      ],
      "metadata": {
        "id": "R3sOfAyJCyOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Load the Excel file, specifying \"Yes\" as a true value and \"No\" as a false value.\n",
        "\n",
        "# Load file with Yes as a True value and No as a False value\n",
        "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
        "                              dtype={\"HasDebt\": bool,\n",
        "                              \"AttendedBootCampYesNo\": bool},\n",
        "                              true_values=[\"Yes\"],\n",
        "                              false_values=[\"No\"])\n",
        "\n",
        "# View the data\n",
        "print(survey_subset.head())\n"
      ],
      "metadata": {
        "id": "27gY3hHuCyK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Load fcc_survey.xlsx, making sure that the Part1StartTime column is parsed as datetime data.\n",
        "View the first few values of the survey_data.Part1StartTime to make sure it contains datetimes.\n",
        "\n",
        "# Load file, with Part1StartTime parsed as datetime data\n",
        "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                            parse_dates=[\"Part1StartTime\"])\n",
        "\n",
        "# Print first few values of Part1StartTime\n",
        "print(survey_data.Part1StartTime.head())\n"
      ],
      "metadata": {
        "id": "mZRJxl0mIj_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a dictionary, datetime_cols indicating that the new column Part2Start should consist of Part2StartDate and Part2StartTime.\n",
        "Load the survey response file, supplying the dictionary to the parse_dates argument to create a new Part2Start column.\n",
        "View summary statistics about the new Part2Start column with the describe() method.\n",
        "\n",
        "# Create dict of columns to combine into new datetime column\n",
        "datetime_cols = {\"Part2Start\": [\"Part2StartDate\",\n",
        "                                \"Part2StartTime\"]}\n",
        "\n",
        "# Load file, supplying the dict to parse_dates\n",
        "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
        "                            parse_dates=datetime_cols)\n",
        "\n",
        "# View summary statistics about Part2Start\n",
        "print(survey_data.Part2Start.describe())\n"
      ],
      "metadata": {
        "id": "JC53m6yHtcHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Parse Part2EndTime using pd.to_datetime(), the format keyword argument, and the format string you just identified. Assign the result back to the Part2EndTime column.\n",
        "\n",
        "# Parse datetimes and assign result back to Part2EndTime\n",
        "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"],\n",
        "                                             format=\"%m%d%Y %H:%M:%S\")\n",
        "\n",
        "\n",
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Print the head of Part2EndTime to confirm the column now contains datetime values.\n",
        "\n",
        "# Parse datetimes and assign result back to Part2EndTime\n",
        "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"],\n",
        "                                             format=\"%m%d%Y %H:%M:%S\")\n",
        "\n",
        "# Print first few values of Part2EndTime\n",
        "print(survey_data.Part2EndTime.head())"
      ],
      "metadata": {
        "id": "TLX1N7kttcEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "0 XP\n",
        "2\n",
        "Import the create_engine() function from sqlalchemy.\n",
        "\n",
        "# Import sqlalchemy's create_engine() function\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Use create_engine() to make a database engine for data.db.\n",
        "Run the last line of code to show the names of the tables in the database.\n",
        "\n",
        "\n",
        "# Import sqlalchemy's create_engine() function\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create the database engine\n",
        "engine = create_engine(\"sqlite:///data.db\")\n",
        "\n",
        "# View the tables in the database\n",
        "print(engine.table_names())"
      ],
      "metadata": {
        "id": "ABxu5qFOsgp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Use read_sql() to load the hpd311calls table by name, without any SQL.\n",
        "\n",
        "\n",
        "# Load libraries\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create the database engine\n",
        "engine = create_engine('sqlite:///data.db')\n",
        "\n",
        "# Load hpd311calls without any SQL\n",
        "hpd_calls = pd.read_sql(\"hpd311calls\", engine)\n",
        "\n",
        "# View the first few rows of data\n",
        "print(hpd_calls.head())\n",
        "\n",
        "\n",
        "\n",
        "Use read_sql() and a SELECT * ... SQL query to load the entire weather table.\n",
        "\n",
        "\n",
        "# Create the database engine\n",
        "engine = create_engine(\"sqlite:///data.db\")\n",
        "\n",
        "# Create a SQL query to load the entire weather table\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "  FROM weather;\n",
        "\"\"\"\n",
        "\n",
        "# Load weather with the SQL query\n",
        "weather = pd.read_sql(query, engine)\n",
        "\n",
        "# View the first few rows of data\n",
        "print(weather.head())"
      ],
      "metadata": {
        "id": "QGnrYAPxsgmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a database engine for data.db.\n",
        "Write a SQL query that SELECTs the date, tmax, and tmin columns from the weather table.\n",
        "Make a dataframe by passing the query and engine to read_sql() and assign the resulting dataframe to temperatures.\n",
        "\n",
        "# Create database engine for data.db\n",
        "engine = create_engine('sqlite:///data.db')\n",
        "\n",
        "# Write query to get date, tmax, and tmin from weather\n",
        "query = \"\"\"\n",
        "SELECT date,\n",
        "       tmax,\n",
        "       tmin\n",
        "  FROM weather;\n",
        "\"\"\"\n",
        "\n",
        "# Make a dataframe by passing query and engine to read_sql()\n",
        "temperatures = pd.read_sql(query, engine)\n",
        "\n",
        "# View the resulting dataframe\n",
        "print(temperatures)\n"
      ],
      "metadata": {
        "id": "DOv3TfGGsghL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a database engine for data.db.\n",
        "Write a SQL query that SELECTs the date, tmax, and tmin columns from the weather table.\n",
        "Make a dataframe by passing the query and engine to read_sql() and assign the resulting dataframe to temperatures.\n",
        "\n",
        "\n",
        "# Create database engine for data.db\n",
        "engine = create_engine('sqlite:///data.db')\n",
        "\n",
        "# Write query to get date, tmax, and tmin from weather\n",
        "query = \"\"\"\n",
        "SELECT date,\n",
        "       tmax,\n",
        "       tmin\n",
        "  FROM weather;\n",
        "\"\"\"\n",
        "\n",
        "# Make a dataframe by passing query and engine to read_sql()\n",
        "temperatures = pd.read_sql(query, engine)\n",
        "\n",
        "# View the resulting dataframe\n",
        "print(temperatures)"
      ],
      "metadata": {
        "id": "giBeEq7Esgdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a query that selects all columns of records in hpd311calls that have 'SAFETY' as their complaint_type.\n",
        "Use read_sql() to query the database and assign the result to the variable safety_calls.\n",
        "Run the last section of code to create a graph of safety call counts in each borough.\n",
        "\n",
        "# Create query to get hpd311calls records about safety\n",
        "query = \"\"\"SELECT *\n",
        "\t\t     FROM hpd311calls\n",
        "            WHERE complaint_type = 'SAFETY';\"\"\"\n",
        "\n",
        "# Query the database and assign result to safety_calls\n",
        "safety_calls = pd.read_sql(query, engine)\n",
        "\n",
        "# Graph the number of safety calls by borough\n",
        "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
        "call_counts.plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3gvSziEAsgbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Instructions\n",
        "100 XP\n",
        "Create a query that selects records in weather where tmax is less than or equal to 32 degrees OR snow is greater than or equal to 1 inch.\n",
        "Use read_sql() to query the database and assign the result to the variable wintry_days.\n",
        "View summary statistics with the describe() method to make sure all records in the dataframe meet the given criteria.\n",
        "\n",
        "\n",
        "# Create query for records with max temps <= 32 or snow >= 1\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "  FROM weather\n",
        " WHERE tmax <= 32\n",
        "    OR snow >= 1;\n",
        "\"\"\"\n",
        "\n",
        "# Query database and assign result to wintry_days\n",
        "wintry_days = pd.read_sql(query, engine)\n",
        "\n",
        "# View summary stats about the temperatures\n",
        "print(wintry_days.describe())"
      ],
      "metadata": {
        "id": "huGH_2kbsgYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YF6KUPO0sgVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjhpdVKDsgTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvgIXOIUsgIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjDqZER9sgBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNfW3_XMsf-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}